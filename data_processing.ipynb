{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephen King Novel NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.714258Z",
     "start_time": "2017-11-06T20:42:19.988964Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.util import ngrams\n",
    "import operator\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn import datasets\n",
    "import json\n",
    "import spacy \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "from config import user_name,password,ip\n",
    "from epub_conversion.utils import open_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.729961Z",
     "start_time": "2017-11-06T20:42:23.717703Z"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "stoplist += ['.', ',', '(', ')', \"'\", '\"']\n",
    "#stoplist = set(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.737117Z",
     "start_time": "2017-11-06T20:42:23.732954Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    text = row['content'].lower()\n",
    "    text = text.strip('\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.753865Z",
     "start_time": "2017-11-06T20:42:23.739690Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def book_word_count(book,n,stoplist=stoplist):\n",
    "    text = clean_text(book)\n",
    "    words = [''.join(words) for words in text.split()]\n",
    "    title = book['title']\n",
    "    counter = Counter()\n",
    "    n = n\n",
    "    words = [w for w in words if w not in stoplist]\n",
    "    bigrams = ngrams(words, n)\n",
    "    counter += Counter(bigrams)\n",
    "    sorted_counter = sorted(counter.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    return title, sorted_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T21:26:42.260226Z",
     "start_time": "2017-11-04T21:26:42.253791Z"
    }
   },
   "source": [
    "### Count Vectorizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.772628Z",
     "start_time": "2017-11-06T20:42:23.756541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def book_cv(dtbooks,stoplist):\n",
    "    cv = CountVectorizer(stop_words=stoplist,token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    print(type(dtbooks[0]))\n",
    "    cv.fit(dtbooks)\n",
    "    x = cv.transform(dtbooks)\n",
    "    x_back = x.toarray()\n",
    "    df = pd.DataFrame(x_back, columns=cv.get_feature_names())\n",
    "    counts = cv.transform(dtbooks).transpose()\n",
    "    print(counts.shape)\n",
    "    corpus = matutils.Sparse2Corpus(counts)\n",
    "    id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "    return df,corpus,id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:47:59.707063Z",
     "start_time": "2017-11-06T20:47:59.697915Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "#     topic_words = []\n",
    "#     for r in model.components_:\n",
    "#         a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:7]\n",
    "#         topic_words.append([books[e[1]-1] for e in a])\n",
    "#     return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:23.804753Z",
     "start_time": "2017-11-06T20:42:23.790167Z"
    }
   },
   "outputs": [],
   "source": [
    "# def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "#     for topic_idx, topic in enumerate(H):\n",
    "#         print(\"Topic %d:\" % (topic_idx))\n",
    "#         print(\" \".join([feature_names[i]\n",
    "#                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "#         top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "#         for doc_index in top_doc_indices:\n",
    "#             print(documents[doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:02:43.036020Z",
     "start_time": "2017-11-06T21:02:43.031373Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "       token = token.lower()\n",
    "    return token.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:36.711669Z",
     "start_time": "2017-11-06T20:42:36.702170Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    document = ' '.join([i for i in document.split() if i not in stoplist])\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:36.925476Z",
     "start_time": "2017-11-06T20:42:36.915497Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_names(document):\n",
    "    names = []\n",
    "    sentences = ie_preprocess(document)\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    names.append(' '.join([c[0] for c in chunk]))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:03:35.902197Z",
     "start_time": "2017-11-06T21:03:35.893440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:03:44.253562Z",
     "start_time": "2017-11-06T21:03:44.244394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary from all books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:37.445913Z",
     "start_time": "2017-11-06T20:42:37.441968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book_list = []\n",
    "# book_dict = {}\n",
    "# path = \"/Users/xavier/dev/metis/fletcher/books/\"\n",
    "# for file in os.listdir(path):\n",
    "#     if file.endswith(\".txt\"):\n",
    "#         clean_name = file.replace(\" - Stephen King.txt\",\"\")\n",
    "#         book_dict[clean_name] = open(path+file, \"r\").read()\n",
    "#         book_list.append(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:37.680158Z",
     "start_time": "2017-11-06T20:42:37.670613Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Insert the books into mongo db\n",
    "# clean_list = []\n",
    "# for k,v in book_dict.items():\n",
    "#     try:\n",
    "#         year = re.search(\"[Cc]opyright ©\\s*.*_*(\\d{4}).*Stephen King|[Cc]opyright ©\\s.*Stephen King.*_*(\\d{4})|[Cc]opyright ©\\s*.*_*(\\d{4}).*Richard Bachman|[Cc]opyright ©\\s.*Richard Bachman.*_*(\\d{4})\",v).group(0) # get copyright year from book text\n",
    "#         year = re.search(\"(\\d{4})\",year).group(0)\n",
    "#     except:\n",
    "#         year = \"\"\n",
    "#     try:\n",
    "#         isbn = re.search(\".*ISBN+:*(\\d*.*)\",v)[1].split(\" \")\n",
    "#         isbn = max(isbn, key=len)\n",
    "#     except:\n",
    "#         isbn = \"\"\n",
    "#     try:\n",
    "#         start = v.find('******start_of_file******')+25\n",
    "#         end = v.find('******end_of_file******')\n",
    "#         text = v[start:end]\n",
    "#     except:\n",
    "#         text = \"\"\n",
    "        \n",
    "#     doc = {\"title\":k,\"year\":year,'isbn':isbn,\"content\":text}\n",
    "#     clean_list.append(doc)\n",
    "#     #print(doc['title'],doc['isbn'])\n",
    "#     #print(doc['title'],doc['year'])\n",
    "#     #db.books.insert_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:37.841548Z",
     "start_time": "2017-11-06T20:42:37.837751Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:37.979253Z",
     "start_time": "2017-11-06T20:42:37.975834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #pd.DataFrame(a, index=['i',])\n",
    "# df = pd.DataFrame(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.165391Z",
     "start_time": "2017-11-06T20:42:38.162477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.515917Z",
     "start_time": "2017-11-06T20:42:38.292274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.534560Z",
     "start_time": "2017-11-06T20:42:38.519112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content', 'isbn', 'title', 'year']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.724504Z",
     "start_time": "2017-11-06T20:42:38.677469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>isbn</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d by “Duel”\\n\\nJoe Hill and Stephen King\\n\\n\\n...</td>\n",
       "      <td>9780062215956</td>\n",
       "      <td>Throttle</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TS\\n\\n\\n\\nCover Page\\n\\nTitle Page\\n\\n\\n\\nIntr...</td>\n",
       "      <td>978-0-385-52884-9</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this Scribner eBook.\\n\\n\\n\\n* * *\\n\\n\\n\\nSign...</td>\n",
       "      <td>0-7432-0467-0</td>\n",
       "      <td>Riding the Bullet</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Page\\n\\nCopyright Page\\n\\nDedication\\n\\n\\n\\n\\...</td>\n",
       "      <td>978-1-101-13813-7</td>\n",
       "      <td>Roadwork</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dication\\n\\nIntroduction\\n\\nAuthor’s Note\\n\\n\\...</td>\n",
       "      <td>978-0-385-52822-1</td>\n",
       "      <td>Salem's Lot</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               isbn  \\\n",
       "0  d by “Duel”\\n\\nJoe Hill and Stephen King\\n\\n\\n...      9780062215956   \n",
       "1  TS\\n\\n\\n\\nCover Page\\n\\nTitle Page\\n\\n\\n\\nIntr...  978-0-385-52884-9   \n",
       "2   this Scribner eBook.\\n\\n\\n\\n* * *\\n\\n\\n\\nSign...      0-7432-0467-0   \n",
       "3   Page\\n\\nCopyright Page\\n\\nDedication\\n\\n\\n\\n\\...  978-1-101-13813-7   \n",
       "4  dication\\n\\nIntroduction\\n\\nAuthor’s Note\\n\\n\\...  978-0-385-52822-1   \n",
       "\n",
       "               title  year  \n",
       "0           Throttle  2009  \n",
       "1        Night Shift  1976  \n",
       "2  Riding the Bullet  2000  \n",
       "3           Roadwork  1981  \n",
       "4        Salem's Lot  1975  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.874772Z",
     "start_time": "2017-11-06T20:42:38.871157Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = df.iloc[28]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:38.992377Z",
     "start_time": "2017-11-06T20:42:38.988900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df['content'] = df.content.apply(lambda x: x.lower())\n",
    "# df['content'] = df.content.apply(lambda x: x.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:39.130404Z",
     "start_time": "2017-11-06T20:42:39.126863Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words = [''.join(words) for words in gs_text.split()]\n",
    "# vectorizer = TfidfVectorizer(stop_words=stop, ngram_range=(1))\n",
    "# doc_vectors = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character extraction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:57.698112Z",
     "start_time": "2017-11-06T20:42:39.454399Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = extract_names(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:57.712069Z",
     "start_time": "2017-11-06T20:42:57.700655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Penguin Book',\n",
       " 'Stephen King',\n",
       " 'Penguin Putnam',\n",
       " 'Penguin Books',\n",
       " 'Penguin Putnam',\n",
       " 'Penguin Putnam',\n",
       " 'STEPHEN',\n",
       " 'Carrie',\n",
       " 'Salem',\n",
       " 'Christine Pet Sematary Cycle Werewolf',\n",
       " 'Peter Straub',\n",
       " 'Dolores Claiborne Insomnia Rose Madder',\n",
       " 'Wizard Glass Bag Bones',\n",
       " 'Tom Gordon Dreamcatcher Black House',\n",
       " 'Peter Straub',\n",
       " 'Skeleton Crew',\n",
       " 'Atlantis Everything',\n",
       " 'Eventual SCREENPLAYS Creepshow Cat',\n",
       " 'Eye Silver Bullet Maximum Overdrive Pet Sematary Golden',\n",
       " 'Century',\n",
       " 'Who',\n",
       " 'Merrys Pippins',\n",
       " 'Max Yasgur',\n",
       " 'Great Woodstock Music Festival',\n",
       " 'Gandalfs',\n",
       " 'Tolkien',\n",
       " 'Stephen Donaldson',\n",
       " 'Terry Brooks',\n",
       " 'Tolkien',\n",
       " 'Tolkien',\n",
       " 'Tricky Dick Nixon',\n",
       " 'Mr. Tolkien',\n",
       " 'Look',\n",
       " 'Stevie',\n",
       " 'Nineteen',\n",
       " 'Bob Seger',\n",
       " 'Patrol Boy',\n",
       " 'Bad Lieutenant',\n",
       " 'Patrol Boy',\n",
       " 'Mine',\n",
       " 'Stephen',\n",
       " 'God',\n",
       " 'Tolkien',\n",
       " 'Pall Malls',\n",
       " 'Patrol Boy',\n",
       " 'Maine',\n",
       " 'Sergio Leone',\n",
       " 'Bad',\n",
       " 'Ugly',\n",
       " 'Tolkien',\n",
       " 'Leone',\n",
       " 'Clint',\n",
       " 'Lee Van Cleef',\n",
       " 'Wizard Glass',\n",
       " 'Leone',\n",
       " 'Phoenix',\n",
       " 'Seems',\n",
       " 'Patrol Boy',\n",
       " 'From Buick',\n",
       " 'Dearborn',\n",
       " 'Michigan',\n",
       " 'Dark Tower',\n",
       " 'Varney Vampire',\n",
       " 'Tower',\n",
       " 'Beams',\n",
       " 'Tower',\n",
       " 'Dark Tower',\n",
       " 'Gramma',\n",
       " 'Outside',\n",
       " 'Me',\n",
       " 'Single',\n",
       " 'Marsha DiFilippo',\n",
       " 'Texas Florida',\n",
       " 'Chussit',\n",
       " 'Patrol Boy',\n",
       " 'Volume Five',\n",
       " 'Stephen',\n",
       " 'Dark Tower',\n",
       " 'Bullshit Rule',\n",
       " 'Dark',\n",
       " 'Tower',\n",
       " 'Dark Tower',\n",
       " 'Chaucer',\n",
       " 'Charles Dickens',\n",
       " 'Stephen',\n",
       " 'Volume Five',\n",
       " 'Volume Six',\n",
       " 'Volume Seven',\n",
       " 'Roland',\n",
       " 'God',\n",
       " 'Roland',\n",
       " 'Dark Tower',\n",
       " 'Dark Tower',\n",
       " 'Thomas',\n",
       " 'Wolfe Look Homeward',\n",
       " 'Had',\n",
       " 'Man Jesus',\n",
       " 'Philistine',\n",
       " 'God',\n",
       " 'Water',\n",
       " 'God',\n",
       " 'Jericho Hill',\n",
       " 'Weren',\n",
       " 'Cort',\n",
       " 'Old Mother',\n",
       " 'Silva',\n",
       " 'Man Jesus',\n",
       " 'Him',\n",
       " 'Algul Siento',\n",
       " 'Blue Haven Heaven',\n",
       " 'Manni',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Zoltan',\n",
       " 'Screw',\n",
       " 'Brown',\n",
       " 'Prayer',\n",
       " 'Lord',\n",
       " 'Brown',\n",
       " 'Thought',\n",
       " 'Did',\n",
       " 'Brown',\n",
       " 'Zoltan',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Zoltan',\n",
       " 'Screw',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'God',\n",
       " 'God',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Pappa',\n",
       " 'Brown',\n",
       " 'Zoltan',\n",
       " 'Tull',\n",
       " 'Sheemie',\n",
       " 'Brown',\n",
       " 'Didn',\n",
       " 'Tull',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Dinner',\n",
       " 'Brown',\n",
       " 'Roasted',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Garlan',\n",
       " 'Zoltan',\n",
       " 'Tull',\n",
       " 'Brown',\n",
       " 'Came',\n",
       " 'Pappa',\n",
       " 'Doc',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Tull',\n",
       " 'Tull',\n",
       " 'Tull',\n",
       " 'Hey Jude',\n",
       " 'Jude',\n",
       " 'Rub',\n",
       " 'Might',\n",
       " 'Charlie',\n",
       " 'Watch Me',\n",
       " 'Mouths',\n",
       " 'Beer',\n",
       " 'Don',\n",
       " 'Bread',\n",
       " 'Peddler',\n",
       " 'Moon',\n",
       " 'High Speech Gilead',\n",
       " 'High Speech',\n",
       " 'Numbed',\n",
       " 'Don',\n",
       " 'Reap',\n",
       " 'Mice',\n",
       " 'Nort',\n",
       " 'Jubal',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Nort',\n",
       " 'Zachary',\n",
       " 'Amy Feldon',\n",
       " 'Aunt Mill',\n",
       " 'Allie',\n",
       " 'Tull',\n",
       " 'Wake',\n",
       " 'Don',\n",
       " 'Mistuh Norton',\n",
       " 'Aunt',\n",
       " 'Mill',\n",
       " 'Sheb',\n",
       " 'Aunt Mill',\n",
       " 'Aunt Mill',\n",
       " 'Thunder',\n",
       " 'Sheb',\n",
       " 'Sheb',\n",
       " 'Nort',\n",
       " 'Nort',\n",
       " 'Sheb',\n",
       " 'Allie',\n",
       " 'Nort',\n",
       " 'Nort',\n",
       " 'Hello',\n",
       " 'Allie',\n",
       " 'Kennerly',\n",
       " 'Sheb',\n",
       " 'Nort',\n",
       " 'Allie',\n",
       " 'Nort',\n",
       " 'Dim',\n",
       " 'Sooner',\n",
       " 'Life',\n",
       " 'Nort',\n",
       " 'Allie',\n",
       " 'Nort',\n",
       " 'Suppose',\n",
       " 'Land Death',\n",
       " 'Don',\n",
       " 'Walter',\n",
       " 'Dim',\n",
       " 'Sooner',\n",
       " 'Allie',\n",
       " 'Don',\n",
       " 'Nort',\n",
       " 'Kennerly',\n",
       " 'Someone',\n",
       " 'Hit',\n",
       " 'Kennerly',\n",
       " 'Soobie',\n",
       " 'Soobie',\n",
       " 'Ain',\n",
       " 'Kennerly',\n",
       " 'Ain',\n",
       " 'Book',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Allie',\n",
       " 'Kennerly',\n",
       " 'Soobie',\n",
       " 'Kennerly',\n",
       " 'Allie',\n",
       " 'Sheb',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Sheb',\n",
       " 'Spittle',\n",
       " 'Sheb',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Sheb',\n",
       " 'Allie',\n",
       " 'Broken',\n",
       " 'Didn',\n",
       " 'Mejis',\n",
       " 'Mejis',\n",
       " 'Clean',\n",
       " 'Susan',\n",
       " 'Sheb',\n",
       " 'Eldred Jonas',\n",
       " 'Coffin Hunter',\n",
       " 'Sheb',\n",
       " 'Sabbath Tull',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Kennerly',\n",
       " 'Castner',\n",
       " 'Allie',\n",
       " 'Sylvia Pittston',\n",
       " 'Mejis',\n",
       " 'Sylvia Pittston',\n",
       " 'Good Book',\n",
       " 'Daniel',\n",
       " 'David',\n",
       " 'Bathsheba',\n",
       " 'Samson',\n",
       " 'St. Paul',\n",
       " 'Damascus',\n",
       " 'Mary Golgotha.',\n",
       " 'Jezebel',\n",
       " 'Ahaz',\n",
       " 'O',\n",
       " 'Jesus',\n",
       " 'Lord',\n",
       " 'Star Wormword',\n",
       " 'LeMark',\n",
       " 'Lord',\n",
       " 'Watch Me',\n",
       " 'Jesus',\n",
       " 'Walter',\n",
       " 'Allie',\n",
       " 'Pittston',\n",
       " 'Jesus Savior',\n",
       " 'Jesus',\n",
       " 'Jonson',\n",
       " 'Lord Flies Serpents',\n",
       " 'Jonson',\n",
       " 'Jonson',\n",
       " 'Wit',\n",
       " 'Will',\n",
       " 'Main Street',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Are',\n",
       " 'Sylvia Pittston',\n",
       " 'Pittston',\n",
       " 'Tongue',\n",
       " 'Antichrist.',\n",
       " 'Don',\n",
       " 'Don',\n",
       " 'Eye',\n",
       " 'Answer',\n",
       " 'Mountains',\n",
       " 'Crimson King',\n",
       " 'Kennerly',\n",
       " 'Ahead',\n",
       " 'Kennerly',\n",
       " 'Soobie',\n",
       " 'Kennerly',\n",
       " 'Kennerly',\n",
       " 'Soobie',\n",
       " 'Soobie',\n",
       " 'Allie',\n",
       " 'Sheb',\n",
       " 'God Tull',\n",
       " 'Allie',\n",
       " 'Allie',\n",
       " 'Sheb',\n",
       " 'Sheb',\n",
       " 'Sheb',\n",
       " 'Sheb Allie',\n",
       " 'Sylvia Pittston',\n",
       " 'Sheb',\n",
       " 'Aunt Mill',\n",
       " 'Sylvia Pittston',\n",
       " 'Eye Hand',\n",
       " 'Kennerly',\n",
       " 'Amy Feldon',\n",
       " 'Sylvia Pittston',\n",
       " 'Tull',\n",
       " 'None',\n",
       " 'Sheb',\n",
       " 'Kennerly',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Brown',\n",
       " 'Walk',\n",
       " 'Allie',\n",
       " 'Cort',\n",
       " 'Susan',\n",
       " 'Mejis',\n",
       " 'Cort',\n",
       " 'High Speech',\n",
       " 'Cort',\n",
       " 'John Chambers',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Zorro',\n",
       " 'Shaw',\n",
       " 'Times',\n",
       " 'Jake',\n",
       " 'Shaw',\n",
       " 'Who',\n",
       " 'Bama',\n",
       " 'Allie',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Tower',\n",
       " 'Tower',\n",
       " 'Chussit',\n",
       " 'Earth Science',\n",
       " 'Geography',\n",
       " 'None',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake Chambers',\n",
       " 'Lanes',\n",
       " 'Clay Blaisdell Western',\n",
       " 'Network',\n",
       " 'Bloomie',\n",
       " 'Shaw',\n",
       " 'Jake',\n",
       " 'Cadillac',\n",
       " 'Jake',\n",
       " 'Kiss',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Cort',\n",
       " 'Susan',\n",
       " 'Marten',\n",
       " 'Jonas',\n",
       " 'Susan',\n",
       " 'Drop',\n",
       " 'Clean Sea',\n",
       " 'Traveller',\n",
       " 'Rest',\n",
       " 'See',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Old',\n",
       " 'Old Mother',\n",
       " 'East Wing',\n",
       " 'Cuthbert Jamie',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Demon',\n",
       " 'Fathoms',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Off',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Marten',\n",
       " 'Round Table',\n",
       " 'Jake',\n",
       " 'Arthur Eld',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Marten',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'Cuthbert',\n",
       " 'Bert',\n",
       " 'Cort',\n",
       " 'Back Courts',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Aren',\n",
       " 'Davey',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Tears',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Roland',\n",
       " 'Roland',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'Roland',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cuthbert Allgood',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Roland',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Cort',\n",
       " 'Roland',\n",
       " 'Cuthbert',\n",
       " 'Hax',\n",
       " 'Bert',\n",
       " 'Guards',\n",
       " 'Guard',\n",
       " 'Don',\n",
       " 'Don',\n",
       " 'Maggie',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Someone',\n",
       " 'Cuthbert',\n",
       " 'Guards',\n",
       " 'Farson',\n",
       " 'Guard',\n",
       " 'Guard',\n",
       " 'Guard',\n",
       " 'Soldier',\n",
       " 'Guard',\n",
       " 'Taunton',\n",
       " 'Guard',\n",
       " 'Will',\n",
       " 'Guard',\n",
       " 'Hax',\n",
       " 'Soldier',\n",
       " 'Guard',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Hax',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Hax',\n",
       " 'Guard',\n",
       " 'Robeson',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Steven',\n",
       " 'Deschain',\n",
       " 'Hendrickson',\n",
       " 'Roland',\n",
       " 'Roland',\n",
       " 'Cuthbert',\n",
       " 'Does',\n",
       " 'Treason',\n",
       " 'Taunton',\n",
       " 'Cuthbert Vannay',\n",
       " 'Deschain',\n",
       " 'Father',\n",
       " 'Roland',\n",
       " 'Cook',\n",
       " 'Sooner',\n",
       " 'Roland',\n",
       " 'Hax',\n",
       " 'Susan',\n",
       " 'Oedipus',\n",
       " 'Farson',\n",
       " 'Gallows Hill Taunton Road',\n",
       " 'Cuthbert',\n",
       " 'Gilead',\n",
       " 'Cuthbert',\n",
       " 'Gallows Hill',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Barony',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Roland',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Roland',\n",
       " 'Taunton',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Hax',\n",
       " 'Cuthbert',\n",
       " 'Hax',\n",
       " 'Hax',\n",
       " 'Hax',\n",
       " 'Bert',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Charles',\n",
       " 'Charles',\n",
       " 'Charles Charles',\n",
       " 'Hax',\n",
       " 'Guards Watch',\n",
       " 'Charles',\n",
       " 'Charles',\n",
       " 'Hax',\n",
       " 'Hax',\n",
       " 'Cuthbert',\n",
       " 'Roland',\n",
       " 'Bert',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Bert',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Cuthbert',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Vannay',\n",
       " 'Allie',\n",
       " 'Tull',\n",
       " 'Allie',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Cuthbert',\n",
       " 'Cuthbert',\n",
       " 'Far',\n",
       " 'Susan Delgado',\n",
       " 'Susan',\n",
       " 'Susan',\n",
       " 'Hey Jude',\n",
       " 'Jake',\n",
       " 'Roland',\n",
       " 'Mejis',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Susan Delgado',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'See',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Don',\n",
       " 'Jake',\n",
       " 'Cuthbert',\n",
       " 'Bert',\n",
       " 'Jake',\n",
       " 'Sylvia Pittston',\n",
       " 'Pittston',\n",
       " 'Jake',\n",
       " 'Cort',\n",
       " 'Gods',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Marten',\n",
       " 'Will',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Narcissus',\n",
       " 'Sunlight',\n",
       " 'Coherent',\n",
       " 'Susan',\n",
       " 'Make',\n",
       " 'Susan',\n",
       " 'Susan Delgado',\n",
       " 'Don',\n",
       " 'Sterile',\n",
       " 'Already',\n",
       " 'Wind',\n",
       " 'Jake',\n",
       " 'Empires',\n",
       " 'Dark Tower',\n",
       " 'Don',\n",
       " 'Marten',\n",
       " 'Sylvia Pittston',\n",
       " 'Tull',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Yar',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Trunks',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Will',\n",
       " 'Don',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Don',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Jake',\n",
       " 'Tower',\n",
       " 'Jake',\n",
       " 'Bible',\n",
       " 'Jesus Moses',\n",
       " 'Ulysses',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Aileen Ritter',\n",
       " 'Cort',\n",
       " 'Hax',\n",
       " 'Slow Mutants',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Patches',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Jeremiah',\n",
       " 'Jake',\n",
       " 'Allie',\n",
       " 'Tull',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Worlds',\n",
       " 'Coffee Thermos',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Jake',\n",
       " 'Fresh Commala',\n",
       " 'Susan Delgado Mejis',\n",
       " 'Ball',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Tet',\n",
       " 'Dinh Gilead',\n",
       " 'Marten',\n",
       " 'Gabrielle Verriss',\n",
       " 'Alan',\n",
       " 'Steven',\n",
       " 'Marten',\n",
       " 'Jake',\n",
       " 'Careful',\n",
       " 'Jake',\n",
       " 'Him',\n",
       " 'Roland',\n",
       " 'Jake',\n",
       " 'Handcar',\n",
       " 'Good',\n",
       " 'Jake',\n",
       " 'Good',\n",
       " 'Jake',\n",
       " 'Great Hall',\n",
       " 'Night Cotillion',\n",
       " 'Tull',\n",
       " 'Next',\n",
       " 'Cuthbert Allgood',\n",
       " 'Susan',\n",
       " 'Goodbye',\n",
       " 'Larchies',\n",
       " 'Jake',\n",
       " 'Roland',\n",
       " 'Da',\n",
       " 'Jake',\n",
       " 'Jake',\n",
       " 'Town',\n",
       " 'Tower',\n",
       " 'Dad',\n",
       " 'Roland',\n",
       " 'Full',\n",
       " 'Marten',\n",
       " 'Don',\n",
       " 'Marten',\n",
       " 'Are',\n",
       " 'Ro',\n",
       " 'Marten',\n",
       " 'Vannay',\n",
       " 'Cuthbert',\n",
       " 'Jamie',\n",
       " 'David',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Cort',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Will',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Cort',\n",
       " 'Jamie',\n",
       " 'Roland',\n",
       " 'Cort',\n",
       " 'High Speech',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'High Speech',\n",
       " 'Cort',\n",
       " 'Rise',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Which',\n",
       " 'Great Hall',\n",
       " 'Cort',\n",
       " 'Marten',\n",
       " 'Cort',\n",
       " 'Roland',\n",
       " 'David',\n",
       " 'David',\n",
       " 'Hai',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'David',\n",
       " 'Great Hall',\n",
       " 'Mark',\n",
       " 'Great Hall',\n",
       " 'Garlan',\n",
       " 'Mohaine Desert',\n",
       " 'Jamie DeCurry',\n",
       " 'Cuthbert Allgood',\n",
       " 'Alain Johns',\n",
       " 'Thomas Whitman',\n",
       " 'Cuthbert',\n",
       " 'Marten',\n",
       " 'Has Cort',\n",
       " 'Cort',\n",
       " 'Eld',\n",
       " 'Cort',\n",
       " 'Steven Deschain',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Did',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'Cuthbert',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Kill',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Gran',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'David',\n",
       " 'Cort',\n",
       " 'Cort',\n",
       " 'Wipe',\n",
       " 'Cort',\n",
       " 'Fools',\n",
       " 'Given',\n",
       " 'Will',\n",
       " 'Cuthbert',\n",
       " 'Thomas Jamie',\n",
       " 'Cuthbert',\n",
       " 'Bert',\n",
       " 'Marten',\n",
       " 'Thomas Jamie',\n",
       " 'Cuthbert',\n",
       " 'Marten',\n",
       " 'Jake',\n",
       " 'Above',\n",
       " 'Slow Mutants',\n",
       " 'None',\n",
       " 'Jake',\n",
       " 'Jesus',\n",
       " 'Jake',\n",
       " 'Slow Mutie',\n",
       " 'Hold',\n",
       " 'Hard',\n",
       " 'Jake',\n",
       " 'Jersey Turnpike',\n",
       " 'Elmer Chambers',\n",
       " 'Sargasso',\n",
       " 'High Speech',\n",
       " 'Subway',\n",
       " 'Gas',\n",
       " 'Vannay',\n",
       " 'Okay',\n",
       " 'Feast Reaptide',\n",
       " 'Jake',\n",
       " 'Mother',\n",
       " 'Simon',\n",
       " 'Mother Says',\n",
       " 'Cuthbert',\n",
       " 'Jamie',\n",
       " 'Jake',\n",
       " 'Yar',\n",
       " 'Roland',\n",
       " 'Man Jesus',\n",
       " 'Hello',\n",
       " 'Tarot',\n",
       " 'Hanged Man',\n",
       " 'Tower',\n",
       " 'Cuthbert',\n",
       " 'Gather',\n",
       " 'Joshua',\n",
       " 'Are',\n",
       " 'Gilead',\n",
       " 'Mockery',\n",
       " 'Tower',\n",
       " 'Roland',\n",
       " 'Hanged Man',\n",
       " 'Na',\n",
       " 'Don',\n",
       " 'Don',\n",
       " 'Tower Hanged Man',\n",
       " 'Volcanoes',\n",
       " 'Okay',\n",
       " 'Dinosaurs',\n",
       " 'Vannay',\n",
       " 'Further',\n",
       " 'Light',\n",
       " 'Tower',\n",
       " 'Shaken',\n",
       " 'Walter',\n",
       " 'Dark Tower',\n",
       " 'Earth',\n",
       " 'Steven',\n",
       " 'Great All',\n",
       " 'Gunslinger',\n",
       " 'Tower',\n",
       " 'Daddy',\n",
       " 'Might',\n",
       " 'Horsehead Nebula',\n",
       " 'God',\n",
       " 'Mohaine Desert',\n",
       " 'Tower',\n",
       " 'Room',\n",
       " 'Afraid',\n",
       " 'Milky Way',\n",
       " 'Cort',\n",
       " 'Ageless',\n",
       " 'Stranger',\n",
       " 'Tower',\n",
       " 'Tower',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Marten',\n",
       " 'Tower',\n",
       " 'Tower',\n",
       " 'Water',\n",
       " 'Oracle',\n",
       " 'Ware',\n",
       " 'Roland',\n",
       " 'Roland',\n",
       " 'Walter',\n",
       " 'Walter',\n",
       " 'Dark Tower',\n",
       " 'Bullshit Factor',\n",
       " 'Scribner',\n",
       " 'Farson',\n",
       " 'John Farson',\n",
       " 'Gilead']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:57.719021Z",
     "start_time": "2017-11-06T20:42:57.714635Z"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = list(stoplist)\n",
    "stoplist.extend(characters)\n",
    "stoplist = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:42:57.726986Z",
     "start_time": "2017-11-06T20:42:57.721965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STEPHEN', 'Aileen Ritter', 'Wolfe Look Homeward', 'Man Jesus', 'under', 'itself', 'didn', 'above', 'Wit', 'Already', 'John Farson', 'Clean', 'Hello', 'Jamie DeCurry', 'is', 'from', 'Shaw', 'Penguin Books', 'Thomas Jamie', 'Damascus', 'Peddler', 'Bathsheba', 'Phoenix', 'Sylvia Pittston', 'than', 'High Speech', 'Susan', 'Bad Lieutenant', 'Further', 'Reap', 'against', 'Old', 'in', 'Cuthbert Allgood', 'Seems', 'your', 'did', 'just', 'Maine', 'Hard', 'don', 'was', 'Coherent', 'with', 'what', 'do', 'Salem', 'Mark', 'they', 'not', 'Answer', 'Jesus', 'down', 'Stephen Donaldson', 'God Tull', 'aren', 'Zoltan', 'Slow Mutants', 'them', 'Mother', 'Chaucer', 'Hendrickson', 'Gather', 'Jake', 'Feast Reaptide', 'Leone', 'Roland', 'very', 'me', 'Milky Way', 'Cook', 'Wind', 'None', 'Scribner', 'Rub', 'Tolkien', 'couldn', 'Coffee Thermos', 'Beer', 'Dinh Gilead', 'Demon', 'Make', 'Lee Van Cleef', 'Has Cort', 'Blue Haven Heaven', 'themselves', 'Worlds', 'Suppose', 'until', 'LeMark', 'Cuthbert Jamie', 'Gandalfs', 'Network', 'Steven', 'Horsehead Nebula', 'Prayer', 'Guards', 'a', 'into', 'Star Wormword', 'Algul Siento', 'High Speech Gilead', 'Mohaine Desert', 'why', 'yourselves', 'her', 'below', 'Jericho Hill', 'Land Death', 'Subway', 'mightn', 'Far', 'all', 'Zachary', 'Ware', 'some', 'Jonas', 'God', 'Lanes', 'nor', 'Water', 'Pappa', 'Hanged Man', 'Spittle', 'Mary Golgotha.', 'John Chambers', 'Barony', 'had', 'ma', 'Vannay', \"'\", 'Who', 'Wizard Glass Bag Bones', 'have', 'Careful', 'Na', 'their', 'when', 'Ro', 'Dearborn', 'if', ',', 'Clint', 'Oedipus', 'Night Cotillion', 'you', 'Sargasso', 'Book', 'Didn', 'Eye Silver Bullet Maximum Overdrive Pet Sematary Golden', 'Had', 'Ahaz', 'Thomas', 'Samson', 'Jersey Turnpike', 'Nort', 'Coffin Hunter', 'Times', 'and', 'more', 'own', 'Fresh Commala', 'Fools', 'Kiss', 've', 'having', 'Guards Watch', 'weren', 'Dim', 'she', 'Silva', 'Da', '.', 'Volume Six', 'Gods', 'Ugly', 'here', 'Bullshit Rule', 'Might', 'Good', 'Bama', 'Tears', 'Christine Pet Sematary Cycle Werewolf', 'Are', 'Slow Mutie', 'doing', 'Main Street', 'Dark Tower', 'Eye Hand', 'Nineteen', 'hers', 'Gunslinger', 'Afraid', 'Traveller', 'Tet', 'Jeremiah', 'him', 'mustn', 'Marsha DiFilippo', 'Round Table', 'St. Paul', 'off', 'Aunt Mill', 'Cort', 'Me', 'Antichrist.', 'i', 'Mistuh Norton', 'Robeson', 'both', 'those', 'Shaken', 'Davey', 'each', 'Arthur Eld', 'before', 'Jamie', 'Zorro', 'Dinner', 'Hold', 'Yar', 'Stranger', 'Room', 'Clay Blaisdell Western', 'Mejis', 'Soldier', 'Does', 'where', 'Great All', 'Skeleton Crew', 'after', 'Ulysses', 'Kennerly', 'Cadillac', 'Eldred Jonas', 'Next', 'ours', 'doesn', 'Weren', 'Alan', 'Atlantis Everything', 'Larchies', 'Gramma', 'Dinosaurs', 'Someone', 't', 'Him', 'same', 'Maggie', 'Full', 'Mr. Tolkien', 'Tricky Dick Nixon', 'Good Book', 'Old Mother', 'Bob Seger', 'Jake Chambers', 'Cuthbert Vannay', 'at', 'Castner', 'Thomas Whitman', 'only', 'for', 'herself', 'Narcissus', 'Volume Seven', 'were', 'our', 'most', 'Came', 'Century', 'Patrol Boy', 'Michigan', 'Moon', 'Handcar', 'by', 'Earth Science', 'ain', 'Marten', 'Mother Says', 'we', 'Tower', 'Mouths', 'Tarot', 'Screw', 'few', 'between', 'Numbed', 'shouldn', 'Mice', 'Cuthbert', 'Sabbath Tull', 'Back Courts', 'Tower Hanged Man', 'Farson', 'other', 'Carrie', 'Taunton', 'Gran', 'an', 'Bert', 'theirs', 'Walk', 'Sooner', 'Drop', 'Lord Flies Serpents', 'Amy Feldon', 'Gilead', 'Penguin Book', 'Hey Jude', 'Watch Me', 'further', 'Charles Charles', 'Ain', 'Steven Deschain', 'Susan Delgado Mejis', 'Empires', 'Rise', 'East Wing', 'this', 'Philistine', 'yourself', 'Rest', 'Did', 'Tom Gordon Dreamcatcher Black House', 'Sterile', 'Jude', 'Pall Malls', 'up', '(', 'Mockery', 'Guard', 'm', 'about', 'haven', 'Eventual SCREENPLAYS Creepshow Cat', 'Bloomie', 'being', 'Bad', 'Eye', 'Father', 'Look', 'Hit', 'my', 'Ahead', 'O', 'who', 'Single', '\"', 'Joshua', 'Daddy', 'Aunt', 'o', 'Roasted', 'or', 'Susan Delgado', 'Merrys Pippins', 'has', 'Light', 'Dad', 'Crimson King', 'but', 'Mine', 'Gabrielle Verriss', 'Charles', 'Volcanoes', 'hasn', 'Oracle', 'Terry Brooks', 'Brown', 'Gallows Hill Taunton Road', 'Elmer Chambers', 'Bread', 'once', 'Allie', 'Wake', 'then', 'Varney Vampire', 'Sunlight', 'its', 'whom', 'Great Woodstock Music Festival', 'Manni', 'these', 'Goodbye', 'Jesus Moses', 'Earth', 'Mountains', 'Lord', 'Pittston', 'See', 'Fathoms', 'as', ')', 'Chussit', 'ourselves', 'Simon', 'to', 'Ageless', 'Dolores Claiborne Insomnia Rose Madder', 'Garlan', 'he', 'd', 'Jezebel', 'Daniel', 'Sheemie', 'which', 're', 'Jesus Savior', 'Geography', 'no', 'himself', 'Bible', 'Doc', 'Kill', 'Dark', 'Ball', 'Sergio Leone', 'now', 'Trunks', 'any', 'so', 'his', 'over', 'Stephen', 'Thought', 'the', 'Given', 'how', 'Stevie', 'Beams', 'Thunder', 'through', 'Hai', 'during', 'Max Yasgur', 'Peter Straub', 'Don', 'won', 'Sheb', 'From Buick', 'Mill', 'can', 'be', 'does', 'Above', 'there', 's', 'been', 'hadn', 'are', 'Aren', 'shan', 'Hax', 'yours', 'Great Hall', 'Volume Five', 'Sheb Allie', 'Walter', 'it', 'Tull', 'Texas Florida', 'Jubal', 'Gas', 'Penguin Putnam', 'Stephen King', 'Wizard Glass', 'Jonson', 'y', 'Treason', 'Patches', 'Which', 'Bullshit Factor', 'should', 'too', 'Deschain', 'am', 'll', 'such', 'Off', 'Charlie', 'David', 'needn', 'wouldn', 'Outside', 'Okay', 'wasn', 'out', 'Alain Johns', 'Will', 'Gallows Hill', 'myself', 'Wipe', 'of', 'Life', 'because', 'Town', 'Soobie', 'Eld', 'Charles Dickens', 'will', 'isn', 'on', 'Tongue', 'while', 'Broken', 'Clean Sea', 'again', 'that'}\n"
     ]
    }
   ],
   "source": [
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:44:23.108462Z",
     "start_time": "2017-11-06T20:42:57.729617Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content'] = df.content.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:44:23.126777Z",
     "start_time": "2017-11-06T20:44:23.111074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>isbn</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[d, by, “, Duel, ”, Joe, Hill, and, Stephen, K...</td>\n",
       "      <td>9780062215956</td>\n",
       "      <td>Throttle</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[TS, Cover, Page, Title, Page, Introduction, b...</td>\n",
       "      <td>978-0-385-52884-9</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[this, Scribner, eBook, ., *, *, *, Sign, up, ...</td>\n",
       "      <td>0-7432-0467-0</td>\n",
       "      <td>Riding the Bullet</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Page, Copyright, Page, Dedication, Part, One,...</td>\n",
       "      <td>978-1-101-13813-7</td>\n",
       "      <td>Roadwork</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[dication, Introduction, Author, ’, s, Note, ’...</td>\n",
       "      <td>978-0-385-52822-1</td>\n",
       "      <td>Salem's Lot</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               isbn  \\\n",
       "0  [d, by, “, Duel, ”, Joe, Hill, and, Stephen, K...      9780062215956   \n",
       "1  [TS, Cover, Page, Title, Page, Introduction, b...  978-0-385-52884-9   \n",
       "2  [this, Scribner, eBook, ., *, *, *, Sign, up, ...      0-7432-0467-0   \n",
       "3  [Page, Copyright, Page, Dedication, Part, One,...  978-1-101-13813-7   \n",
       "4  [dication, Introduction, Author, ’, s, Note, ’...  978-0-385-52822-1   \n",
       "\n",
       "               title  year  \n",
       "0           Throttle  2009  \n",
       "1        Night Shift  1976  \n",
       "2  Riding the Bullet  2000  \n",
       "3           Roadwork  1981  \n",
       "4        Salem's Lot  1975  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omit Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:44:23.133466Z",
     "start_time": "2017-11-06T20:44:23.129284Z"
    }
   },
   "outputs": [],
   "source": [
    "#df['content'] = df.content.apply(lambda x: [word for word in x if word not in stoplist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:44:23.154903Z",
     "start_time": "2017-11-06T20:44:23.137135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>isbn</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[d, by, “, Duel, ”, Joe, Hill, and, Stephen, K...</td>\n",
       "      <td>9780062215956</td>\n",
       "      <td>Throttle</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[TS, Cover, Page, Title, Page, Introduction, b...</td>\n",
       "      <td>978-0-385-52884-9</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[this, Scribner, eBook, ., *, *, *, Sign, up, ...</td>\n",
       "      <td>0-7432-0467-0</td>\n",
       "      <td>Riding the Bullet</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Page, Copyright, Page, Dedication, Part, One,...</td>\n",
       "      <td>978-1-101-13813-7</td>\n",
       "      <td>Roadwork</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[dication, Introduction, Author, ’, s, Note, ’...</td>\n",
       "      <td>978-0-385-52822-1</td>\n",
       "      <td>Salem's Lot</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               isbn  \\\n",
       "0  [d, by, “, Duel, ”, Joe, Hill, and, Stephen, K...      9780062215956   \n",
       "1  [TS, Cover, Page, Title, Page, Introduction, b...  978-0-385-52884-9   \n",
       "2  [this, Scribner, eBook, ., *, *, *, Sign, up, ...      0-7432-0467-0   \n",
       "3  [Page, Copyright, Page, Dedication, Part, One,...  978-1-101-13813-7   \n",
       "4  [dication, Introduction, Author, ’, s, Note, ’...  978-0-385-52822-1   \n",
       "\n",
       "               title  year  \n",
       "0           Throttle  2009  \n",
       "1        Night Shift  1976  \n",
       "2  Riding the Bullet  2000  \n",
       "3           Roadwork  1981  \n",
       "4        Salem's Lot  1975  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:44:23.161016Z",
     "start_time": "2017-11-06T20:44:23.157358Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:18.256611Z",
     "start_time": "2017-11-06T20:44:23.163973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content'] = df.content.apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:18.279604Z",
     "start_time": "2017-11-06T20:46:18.259435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>isbn</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[d, by, “, duel, ”, joe, hill, and, stephen, k...</td>\n",
       "      <td>9780062215956</td>\n",
       "      <td>Throttle</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ts, cover, page, titl, page, introduct, by, j...</td>\n",
       "      <td>978-0-385-52884-9</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[this, scribner, ebook, ., *, *, *, sign, up, ...</td>\n",
       "      <td>0-7432-0467-0</td>\n",
       "      <td>Riding the Bullet</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[page, copyright, page, dedic, part, one, -, n...</td>\n",
       "      <td>978-1-101-13813-7</td>\n",
       "      <td>Roadwork</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[dicat, introduct, author, ’, s, note, ’, sale...</td>\n",
       "      <td>978-0-385-52822-1</td>\n",
       "      <td>Salem's Lot</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               isbn  \\\n",
       "0  [d, by, “, duel, ”, joe, hill, and, stephen, k...      9780062215956   \n",
       "1  [ts, cover, page, titl, page, introduct, by, j...  978-0-385-52884-9   \n",
       "2  [this, scribner, ebook, ., *, *, *, sign, up, ...      0-7432-0467-0   \n",
       "3  [page, copyright, page, dedic, part, one, -, n...  978-1-101-13813-7   \n",
       "4  [dicat, introduct, author, ’, s, note, ’, sale...  978-0-385-52822-1   \n",
       "\n",
       "               title  year  \n",
       "0           Throttle  2009  \n",
       "1        Night Shift  1976  \n",
       "2  Riding the Bullet  2000  \n",
       "3           Roadwork  1981  \n",
       "4        Salem's Lot  1975  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens back to a long string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:19.177386Z",
     "start_time": "2017-11-06T20:46:18.292880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content'] = df.content.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:19.207277Z",
     "start_time": "2017-11-06T20:46:19.180255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>isbn</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d by “ duel ” joe hill and stephen king conten...</td>\n",
       "      <td>9780062215956</td>\n",
       "      <td>Throttle</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ts cover page titl page introduct by john d. m...</td>\n",
       "      <td>978-0-385-52884-9</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this scribner ebook . * * * sign up for our ne...</td>\n",
       "      <td>0-7432-0467-0</td>\n",
       "      <td>Riding the Bullet</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>page copyright page dedic part one - novemb no...</td>\n",
       "      <td>978-1-101-13813-7</td>\n",
       "      <td>Roadwork</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dicat introduct author ’ s note ’ salem ’ s lo...</td>\n",
       "      <td>978-0-385-52822-1</td>\n",
       "      <td>Salem's Lot</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               isbn  \\\n",
       "0  d by “ duel ” joe hill and stephen king conten...      9780062215956   \n",
       "1  ts cover page titl page introduct by john d. m...  978-0-385-52884-9   \n",
       "2  this scribner ebook . * * * sign up for our ne...      0-7432-0467-0   \n",
       "3  page copyright page dedic part one - novemb no...  978-1-101-13813-7   \n",
       "4  dicat introduct author ’ s note ’ salem ’ s lo...  978-0-385-52822-1   \n",
       "\n",
       "               title  year  \n",
       "0           Throttle  2009  \n",
       "1        Night Shift  1976  \n",
       "2  Riding the Bullet  2000  \n",
       "3           Roadwork  1981  \n",
       "4        Salem's Lot  1975  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:00:17.806548Z",
     "start_time": "2017-11-06T21:00:17.801813Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:00:18.951311Z",
     "start_time": "2017-11-06T21:00:18.945021Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T21:00:20.244128Z",
     "start_time": "2017-11-06T21:00:20.235778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dicat introduct author ’ s note ’ salem ’ s lot prologu part one the marsten hous chapter one ben ( i ) chapter two susan ( i ) chapter three the lot ( i ) chapter four danni glick and other chapter five ben ( ii ) chapter six the lot ( ii ) chapter seven matt part two the emperor of ice cream chapter eight ben ( iii ) chapter nine susan ( ii ) chapter ten the lot ( iii ) chapter eleven ben ( iv ) chapter twelv mark chapter thirteen father callahan part three the desert villag chapter fourteen t'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[4][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:25.187476Z",
     "start_time": "2017-11-06T20:46:19.353979Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stoplist)\n",
    "x = cv.fit_transform(books)\n",
    "cv_feature_names = cv.get_feature_names()\n",
    "#x = cv.transform(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:25.196814Z",
     "start_time": "2017-11-06T20:46:25.190358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 58142)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:25.206298Z",
     "start_time": "2017-11-06T20:46:25.202197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_back = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:25.214343Z",
     "start_time": "2017-11-06T20:46:25.209367Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bookDF = pd.DataFrame(x_back, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:25.222806Z",
     "start_time": "2017-11-06T20:46:25.219848Z"
    }
   },
   "outputs": [],
   "source": [
    "# bookDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:31.203781Z",
     "start_time": "2017-11-06T20:46:25.225629Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words=stoplist)\n",
    "x2 = tf.fit_transform(books)\n",
    "tf_feature_names = tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:31.328252Z",
     "start_time": "2017-11-06T20:46:31.206429Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:36.056948Z",
     "start_time": "2017-11-06T20:46:31.330768Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(x2)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle Kmeans Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:36.062710Z",
     "start_time": "2017-11-06T20:46:36.059469Z"
    }
   },
   "outputs": [],
   "source": [
    "# joblib.dump(km, 'doc_cluster.pkl')\n",
    "# km = joblib.load('doc_cluster.pkl')\n",
    "# clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T21:21:31.978018Z",
     "start_time": "2017-11-04T21:21:31.973811Z"
    }
   },
   "source": [
    "#### With Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:46:49.689832Z",
     "start_time": "2017-11-06T20:46:36.065214Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, init='random')\n",
    "fit = nmf.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:48:22.204348Z",
     "start_time": "2017-11-06T20:48:22.103065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "said one look like would back could sam time thought\n",
      "Topic 1:\n",
      "said bill look one like back richi would ben eddi\n",
      "Topic 2:\n",
      "jack said richard wolf look like back one go hand\n",
      "Topic 3:\n",
      "roland said one look like back would hand susan could\n",
      "Topic 4:\n",
      "said one look go back barbi big like would jim\n",
      "Topic 5:\n",
      "jonesi henri one like look back said gray owen kurtz\n",
      "Topic 6:\n",
      "bobbi like said look one garden go back would could\n",
      "Topic 7:\n",
      "said alan one look like would back gaunt go hand\n",
      "Topic 8:\n",
      "said like would look one go back stu man could\n",
      "Topic 9:\n",
      "like look one rosi back could would norman hand thought\n",
      "Topic 10:\n",
      "roland eddi said jake one susannah look like would time\n",
      "Topic 11:\n",
      "clay said tom one look jordan like alic go man\n",
      "Topic 12:\n",
      "said one go like would look back andi dan could\n",
      "Topic 13:\n",
      "johnni said look like one back david go hand steve\n",
      "Topic 14:\n",
      "ralph look loi like said one back hand go could\n",
      "Topic 15:\n",
      "said one like back look go would could know time\n",
      "Topic 16:\n",
      "would peter flagg one said thoma king denni could thought\n",
      "Topic 17:\n",
      "hodg say one think know like go jerom get look\n",
      "Topic 18:\n",
      "loui said jud rachel would back like go one gage\n",
      "Topic 19:\n",
      "lisey scott one like back look said amanda go say\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf,cv_feature_names,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:48:22.002730Z",
     "start_time": "2017-11-06T20:48:06.500105Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=20, init='random').fit(x2)\n",
    "fit2 = nmf2.transform(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:48:22.010880Z",
     "start_time": "2017-11-06T20:48:22.005518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no_top_words = 5\n",
    "# no_top_documents = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:48:22.097391Z",
     "start_time": "2017-11-06T20:48:22.013993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "said one like look back\n",
      "Topic 1:\n",
      "hodg jerom morri bradi say\n",
      "Topic 2:\n",
      "flagg peyna denni peter beson\n",
      "Topic 3:\n",
      "lisey scott amanda dooley one\n",
      "Topic 4:\n",
      "jack richard said counting like\n",
      "Topic 5:\n",
      "abra kid like one danni\n",
      "Topic 6:\n",
      "loui jud rachel gage said\n",
      "Topic 7:\n",
      "ralph loi look like mcgovern\n",
      "Topic 8:\n",
      "bobbi ever said kaz one\n",
      "Topic 9:\n",
      "garrati mcvri stebbin olson said\n",
      "Topic 10:\n",
      "jonesi kurtz duddit henri owen\n",
      "Topic 11:\n",
      "jessi like one would gerald\n",
      "Topic 12:\n",
      "roland eddi jake said susannah\n",
      "Topic 13:\n",
      "tess streeter would ramona said\n",
      "Topic 14:\n",
      "ginelli halleck billi heidi said\n",
      "Topic 15:\n",
      "wesley ur robbi said kindl\n",
      "Topic 16:\n",
      "vinc lemmi said stephani dave\n",
      "Topic 17:\n",
      "becki cal grass demuth tobin\n",
      "Topic 18:\n",
      "perci coffey delacroix wharton said\n",
      "Topic 19:\n",
      "trisha like one back look\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf2,tf_feature_names,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T21:25:35.299501Z",
     "start_time": "2017-11-04T21:25:35.296533Z"
    }
   },
   "source": [
    "#### With Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:14:28.811641Z",
     "start_time": "2017-11-06T20:14:21.252116Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(stop_words=stoplist)\n",
    "x3 = cv2.fit_transform(books)\n",
    "cv2_feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:14:35.933530Z",
     "start_time": "2017-11-06T20:14:28.813962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavier/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T20:14:36.135304Z",
     "start_time": "2017-11-06T20:14:35.936807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "said one like back could\n",
      "Topic 1:\n",
      "said one like back would\n",
      "Topic 2:\n",
      "said one like would time\n",
      "Topic 3:\n",
      "said one back would like\n",
      "Topic 4:\n",
      "said one back like would\n",
      "Topic 5:\n",
      "said like back would one\n",
      "Topic 6:\n",
      "said like one could would\n",
      "Topic 7:\n",
      "said one like could back\n",
      "Topic 8:\n",
      "said back one like would\n",
      "Topic 9:\n",
      "like one said would could\n",
      "Topic 10:\n",
      "said one back would get\n",
      "Topic 11:\n",
      "one said like back would\n",
      "Topic 12:\n",
      "said one like back could\n",
      "Topic 13:\n",
      "said back one could would\n",
      "Topic 14:\n",
      "said like one back could\n",
      "Topic 15:\n",
      "said one like back would\n",
      "Topic 16:\n",
      "one would like said back\n",
      "Topic 17:\n",
      "said one like would back\n",
      "Topic 18:\n",
      "one back said like would\n",
      "Topic 19:\n",
      "said one like would could\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda,cv2_feature_names,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:45:59.353350Z",
     "start_time": "2017-11-02T21:40:37.735808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 17:40:37,750 : INFO : using symmetric alpha at 0.1\n",
      "2017-11-02 17:40:37,751 : INFO : using symmetric eta at 2.13193251154e-06\n",
      "2017-11-02 17:40:37,826 : INFO : using serial LDA version on this node\n",
      "2017-11-02 17:40:58,603 : INFO : running online (multi-pass) LDA training, 10 topics, 10 passes over the supplied corpus of 7 documents, updating model once every 7 documents, evaluating perplexity every 7 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-11-02 17:41:47,044 : INFO : -17.591 per-word bound, 197481.1 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:41:47,046 : INFO : PROGRESS: pass 0, at document #7/7\n",
      "2017-11-02 17:41:48,766 : INFO : topic #7 (0.100): 0.005*\"said\" + 0.005*\"roland\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"jake\" + 0.003*\"one\" + 0.003*\"would\" + 0.002*\"back\" + 0.002*\"susannah\" + 0.002*\"could\"\n",
      "2017-11-02 17:41:48,772 : INFO : topic #1 (0.100): 0.006*\"said\" + 0.004*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"would\" + 0.003*\"like\" + 0.002*\"jake\" + 0.002*\"could\" + 0.002*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:41:48,776 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"said\" + 0.003*\"eddie\" + 0.003*\"jake\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"could\"\n",
      "2017-11-02 17:41:48,781 : INFO : topic #2 (0.100): 0.006*\"roland\" + 0.005*\"one\" + 0.004*\"eddie\" + 0.004*\"said\" + 0.003*\"like\" + 0.002*\"jake\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"susannah\" + 0.002*\"time\"\n",
      "2017-11-02 17:41:48,786 : INFO : topic #3 (0.100): 0.003*\"said\" + 0.003*\"eddie\" + 0.003*\"one\" + 0.003*\"like\" + 0.003*\"roland\" + 0.003*\"would\" + 0.003*\"jake\" + 0.002*\"back\" + 0.002*\"susannah\" + 0.002*\"could\"\n",
      "2017-11-02 17:41:48,803 : INFO : topic diff=3.357278, rho=1.000000\n",
      "2017-11-02 17:42:09,432 : INFO : -13.317 per-word bound, 10205.0 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:42:09,433 : INFO : PROGRESS: pass 1, at document #7/7\n",
      "2017-11-02 17:42:11,406 : INFO : topic #1 (0.100): 0.005*\"said\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"eddie\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"jake\" + 0.002*\"looked\"\n",
      "2017-11-02 17:42:11,412 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"one\" + 0.004*\"said\" + 0.004*\"eddie\" + 0.003*\"jake\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"could\" + 0.002*\"back\" + 0.002*\"time\"\n",
      "2017-11-02 17:42:11,418 : INFO : topic #4 (0.100): 0.003*\"roland\" + 0.003*\"said\" + 0.003*\"eddie\" + 0.002*\"one\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"like\" + 0.002*\"know\" + 0.001*\"jake\" + 0.001*\"gunslinger\"\n",
      "2017-11-02 17:42:11,422 : INFO : topic #3 (0.100): 0.003*\"said\" + 0.002*\"eddie\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"roland\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"back\" + 0.001*\"susannah\" + 0.001*\"could\"\n",
      "2017-11-02 17:42:11,427 : INFO : topic #0 (0.100): 0.004*\"said\" + 0.003*\"gunslinger\" + 0.003*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"roland\" + 0.002*\"man\" + 0.002*\"jake\" + 0.002*\"boy\" + 0.002*\"would\"\n",
      "2017-11-02 17:42:11,442 : INFO : topic diff=1.742990, rho=0.577350\n",
      "2017-11-02 17:42:34,317 : INFO : -12.175 per-word bound, 4625.2 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:42:34,319 : INFO : PROGRESS: pass 2, at document #7/7\n",
      "2017-11-02 17:42:36,180 : INFO : topic #6 (0.100): 0.002*\"roland\" + 0.001*\"one\" + 0.001*\"would\" + 0.001*\"like\" + 0.001*\"said\" + 0.001*\"eddie\" + 0.001*\"thought\" + 0.001*\"jake\" + 0.001*\"time\" + 0.001*\"could\"\n",
      "2017-11-02 17:42:36,186 : INFO : topic #3 (0.100): 0.002*\"said\" + 0.002*\"eddie\" + 0.002*\"one\" + 0.001*\"like\" + 0.001*\"roland\" + 0.001*\"would\" + 0.001*\"jake\" + 0.001*\"back\" + 0.001*\"susannah\" + 0.001*\"could\"\n",
      "2017-11-02 17:42:36,191 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"one\" + 0.005*\"eddie\" + 0.003*\"jake\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:42:36,196 : INFO : topic #7 (0.100): 0.004*\"said\" + 0.004*\"roland\" + 0.002*\"would\" + 0.002*\"eddie\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"jake\" + 0.002*\"susannah\" + 0.002*\"back\" + 0.001*\"could\"\n",
      "2017-11-02 17:42:36,202 : INFO : topic #0 (0.100): 0.004*\"gunslinger\" + 0.004*\"said\" + 0.002*\"one\" + 0.002*\"man\" + 0.002*\"boy\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"roland\"\n",
      "2017-11-02 17:42:36,222 : INFO : topic diff=1.024486, rho=0.500000\n",
      "2017-11-02 17:42:56,616 : INFO : -11.817 per-word bound, 3607.3 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:42:56,617 : INFO : PROGRESS: pass 3, at document #7/7\n",
      "2017-11-02 17:43:01,297 : INFO : topic #3 (0.100): 0.001*\"said\" + 0.001*\"eddie\" + 0.001*\"one\" + 0.001*\"like\" + 0.001*\"roland\" + 0.001*\"would\" + 0.001*\"jake\" + 0.001*\"back\" + 0.001*\"susannah\" + 0.001*\"could\"\n",
      "2017-11-02 17:43:01,303 : INFO : topic #0 (0.100): 0.005*\"gunslinger\" + 0.003*\"said\" + 0.003*\"boy\" + 0.002*\"man\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"jake\" + 0.001*\"black\"\n",
      "2017-11-02 17:43:01,308 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.004*\"one\" + 0.003*\"susannah\" + 0.002*\"like\" + 0.002*\"mia\" + 0.002*\"jake\" + 0.002*\"would\" + 0.002*\"back\"\n",
      "2017-11-02 17:43:01,313 : INFO : topic #9 (0.100): 0.003*\"eddie\" + 0.002*\"said\" + 0.002*\"roland\" + 0.002*\"one\" + 0.002*\"gunslinger\" + 0.001*\"would\" + 0.001*\"like\" + 0.001*\"man\" + 0.001*\"back\" + 0.001*\"thought\"\n",
      "2017-11-02 17:43:01,317 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"jake\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"looked\"\n",
      "2017-11-02 17:43:01,333 : INFO : topic diff=0.634346, rho=0.447214\n",
      "2017-11-02 17:43:23,162 : INFO : -11.680 per-word bound, 3282.0 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:43:23,163 : INFO : PROGRESS: pass 4, at document #7/7\n",
      "2017-11-02 17:43:30,222 : INFO : topic #4 (0.100): 0.001*\"roland\" + 0.001*\"said\" + 0.001*\"eddie\" + 0.001*\"one\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"\n",
      "2017-11-02 17:43:30,227 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.004*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"jake\" + 0.002*\"would\" + 0.002*\"back\"\n",
      "2017-11-02 17:43:30,232 : INFO : topic #9 (0.100): 0.003*\"eddie\" + 0.001*\"gunslinger\" + 0.001*\"said\" + 0.001*\"roland\" + 0.001*\"one\" + 0.001*\"would\" + 0.001*\"like\" + 0.001*\"man\" + 0.001*\"back\" + 0.001*\"balazar\"\n",
      "2017-11-02 17:43:30,238 : INFO : topic #0 (0.100): 0.005*\"gunslinger\" + 0.003*\"said\" + 0.003*\"boy\" + 0.002*\"man\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"black\" + 0.002*\"jake\"\n",
      "2017-11-02 17:43:30,242 : INFO : topic #1 (0.100): 0.002*\"said\" + 0.002*\"roland\" + 0.001*\"one\" + 0.001*\"would\" + 0.001*\"like\" + 0.001*\"eddie\" + 0.001*\"back\" + 0.001*\"could\" + 0.001*\"looked\" + 0.001*\"jake\"\n",
      "2017-11-02 17:43:30,258 : INFO : topic diff=0.406852, rho=0.408248\n",
      "2017-11-02 17:43:53,934 : INFO : -11.615 per-word bound, 3135.8 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:43:53,935 : INFO : PROGRESS: pass 5, at document #7/7\n",
      "2017-11-02 17:44:00,612 : INFO : topic #0 (0.100): 0.005*\"gunslinger\" + 0.003*\"said\" + 0.003*\"boy\" + 0.002*\"man\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"black\" + 0.002*\"jake\"\n",
      "2017-11-02 17:44:00,617 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:44:00,622 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"jake\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:44:00,627 : INFO : topic #3 (0.100): 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:00,631 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.004*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"jake\" + 0.002*\"would\" + 0.002*\"back\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 17:44:00,647 : INFO : topic diff=0.267182, rho=0.377964\n",
      "2017-11-02 17:44:23,423 : INFO : -11.579 per-word bound, 3060.3 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:44:23,424 : INFO : PROGRESS: pass 6, at document #7/7\n",
      "2017-11-02 17:44:30,039 : INFO : topic #7 (0.100): 0.001*\"said\" + 0.001*\"roland\" + 0.001*\"would\" + 0.001*\"eddie\" + 0.001*\"one\" + 0.001*\"like\" + 0.001*\"jake\" + 0.001*\"susannah\" + 0.000*\"back\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:30,044 : INFO : topic #3 (0.100): 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:30,048 : INFO : topic #4 (0.100): 0.000*\"roland\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"\n",
      "2017-11-02 17:44:30,052 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"jake\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:44:30,057 : INFO : topic #6 (0.100): 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"thought\" + 0.000*\"jake\" + 0.000*\"time\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:30,072 : INFO : topic diff=0.179337, rho=0.353553\n",
      "2017-11-02 17:44:52,891 : INFO : -11.559 per-word bound, 3017.7 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:44:52,893 : INFO : PROGRESS: pass 7, at document #7/7\n",
      "2017-11-02 17:44:59,560 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:44:59,566 : INFO : topic #3 (0.100): 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:59,572 : INFO : topic #6 (0.100): 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"thought\" + 0.000*\"jake\" + 0.000*\"time\" + 0.000*\"could\"\n",
      "2017-11-02 17:44:59,578 : INFO : topic #1 (0.100): 0.001*\"said\" + 0.001*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"eddie\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"looked\" + 0.000*\"jake\"\n",
      "2017-11-02 17:44:59,582 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"jake\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:44:59,597 : INFO : topic diff=0.122909, rho=0.333333\n",
      "2017-11-02 17:45:23,074 : INFO : -11.547 per-word bound, 2991.9 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:45:23,075 : INFO : PROGRESS: pass 8, at document #7/7\n",
      "2017-11-02 17:45:29,696 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"jake\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:45:29,701 : INFO : topic #4 (0.100): 0.000*\"roland\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"\n",
      "2017-11-02 17:45:29,706 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:45:29,710 : INFO : topic #1 (0.100): 0.001*\"said\" + 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"eddie\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"looked\" + 0.000*\"jake\"\n",
      "2017-11-02 17:45:29,715 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"back\"\n",
      "2017-11-02 17:45:29,732 : INFO : topic diff=0.085938, rho=0.316228\n",
      "2017-11-02 17:45:52,696 : INFO : -11.539 per-word bound, 2975.4 perplexity estimate based on a held-out corpus of 7 documents with 1304555 words\n",
      "2017-11-02 17:45:52,697 : INFO : PROGRESS: pass 9, at document #7/7\n",
      "2017-11-02 17:45:59,256 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"jake\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:45:59,261 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"back\"\n",
      "2017-11-02 17:45:59,266 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:45:59,270 : INFO : topic #4 (0.100): 0.000*\"roland\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"\n",
      "2017-11-02 17:45:59,275 : INFO : topic #3 (0.100): 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"\n",
      "2017-11-02 17:45:59,291 : INFO : topic diff=0.061250, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "# lda = models.LdaModel(corpus=corp, num_topics=10, id2word=id2word, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:45:59.423758Z",
     "start_time": "2017-11-02T21:45:59.356125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 17:45:59,374 : INFO : topic #0 (0.100): 0.005*\"gunslinger\" + 0.003*\"said\" + 0.003*\"boy\" + 0.003*\"man\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"black\" + 0.001*\"jake\"\n",
      "2017-11-02 17:45:59,380 : INFO : topic #1 (0.100): 0.000*\"said\" + 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"eddie\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"looked\" + 0.000*\"jake\"\n",
      "2017-11-02 17:45:59,384 : INFO : topic #2 (0.100): 0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"back\"\n",
      "2017-11-02 17:45:59,388 : INFO : topic #3 (0.100): 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"\n",
      "2017-11-02 17:45:59,392 : INFO : topic #4 (0.100): 0.000*\"roland\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"\n",
      "2017-11-02 17:45:59,396 : INFO : topic #5 (0.100): 0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"\n",
      "2017-11-02 17:45:59,399 : INFO : topic #6 (0.100): 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"thought\" + 0.000*\"jake\" + 0.000*\"time\" + 0.000*\"could\"\n",
      "2017-11-02 17:45:59,404 : INFO : topic #7 (0.100): 0.000*\"said\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"jake\" + 0.000*\"susannah\" + 0.000*\"back\" + 0.000*\"could\"\n",
      "2017-11-02 17:45:59,408 : INFO : topic #8 (0.100): 0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"jake\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"\n",
      "2017-11-02 17:45:59,414 : INFO : topic #9 (0.100): 0.002*\"eddie\" + 0.001*\"balazar\" + 0.001*\"gunslinger\" + 0.001*\"jack\" + 0.001*\"andolini\" + 0.001*\"henry\" + 0.001*\"odetta\" + 0.001*\"mort\" + 0.001*\"delevan\" + 0.001*\"customs\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"gunslinger\" + 0.003*\"said\" + 0.003*\"boy\" + 0.003*\"man\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"black\" + 0.001*\"jake\"'),\n",
       " (1,\n",
       "  '0.000*\"said\" + 0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"eddie\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"looked\" + 0.000*\"jake\"'),\n",
       " (2,\n",
       "  '0.005*\"said\" + 0.004*\"eddie\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"susannah\" + 0.003*\"mia\" + 0.002*\"like\" + 0.002*\"would\" + 0.002*\"jake\" + 0.002*\"back\"'),\n",
       " (3,\n",
       "  '0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"jake\" + 0.000*\"back\" + 0.000*\"susannah\" + 0.000*\"could\"'),\n",
       " (4,\n",
       "  '0.000*\"roland\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"jake\" + 0.000*\"gunslinger\"'),\n",
       " (5,\n",
       "  '0.006*\"roland\" + 0.005*\"said\" + 0.005*\"eddie\" + 0.005*\"one\" + 0.004*\"jake\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"could\" + 0.003*\"back\" + 0.002*\"susannah\"'),\n",
       " (6,\n",
       "  '0.000*\"roland\" + 0.000*\"one\" + 0.000*\"would\" + 0.000*\"like\" + 0.000*\"said\" + 0.000*\"eddie\" + 0.000*\"thought\" + 0.000*\"jake\" + 0.000*\"time\" + 0.000*\"could\"'),\n",
       " (7,\n",
       "  '0.000*\"said\" + 0.000*\"roland\" + 0.000*\"would\" + 0.000*\"eddie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"jake\" + 0.000*\"susannah\" + 0.000*\"back\" + 0.000*\"could\"'),\n",
       " (8,\n",
       "  '0.006*\"said\" + 0.006*\"roland\" + 0.004*\"one\" + 0.004*\"eddie\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"jake\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"time\"'),\n",
       " (9,\n",
       "  '0.002*\"eddie\" + 0.001*\"balazar\" + 0.001*\"gunslinger\" + 0.001*\"jack\" + 0.001*\"andolini\" + 0.001*\"henry\" + 0.001*\"odetta\" + 0.001*\"mort\" + 0.001*\"delevan\" + 0.001*\"customs\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:45:59.985675Z",
     "start_time": "2017-11-02T21:45:59.427197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "# lda_corpus = lda[corp]\n",
    "# lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:45:59.995215Z",
     "start_time": "2017-11-02T21:45:59.988441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 0.99999309930389702)],\n",
       " [(8, 0.99999659776824634)],\n",
       " [(5, 0.99999504978884213)],\n",
       " [(0, 0.9999867660232642)],\n",
       " [(5, 0.80863682693916183), (9, 0.19135691336193036)],\n",
       " [(5, 0.9999967903172694)],\n",
       " [(8, 0.99999642158611446)]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda_docs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:46:00.002476Z",
     "start_time": "2017-11-02T21:45:59.997923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LdaModel.log_perplexity of <gensim.models.ldamodel.LdaModel object at 0x1a29cd38d0>>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda.log_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:49:53.212960Z",
     "start_time": "2017-11-02T21:49:14.280746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "(2680972, 67)\n"
     ]
    }
   ],
   "source": [
    "# corp2,id2word2 = book_cv(all_text,stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T23:37:39.919136Z",
     "start_time": "2017-11-02T22:59:21.348750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 18:59:21,423 : INFO : using symmetric alpha at 0.1\n",
      "2017-11-02 18:59:21,425 : INFO : using symmetric eta at 3.72999046614e-07\n",
      "2017-11-02 18:59:21,803 : INFO : using serial LDA version on this node\n",
      "2017-11-02 19:01:16,931 : INFO : running online (multi-pass) LDA training, 10 topics, 10 passes over the supplied corpus of 67 documents, updating model once every 67 documents, evaluating perplexity every 67 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-11-02 19:06:27,225 : INFO : -18.245 per-word bound, 310611.4 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:06:27,227 : INFO : PROGRESS: pass 0, at document #67/67\n",
      "2017-11-02 19:06:41,693 : INFO : topic #0 (0.100): 0.005*\"said\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"one\" + 0.002*\"time\" + 0.002*\"could\" + 0.002*\"back\" + 0.002*\"looked\" + 0.002*\"go\" + 0.002*\"way\"\n",
      "2017-11-02 19:06:41,733 : INFO : topic #5 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"time\" + 0.002*\"could\" + 0.002*\"right\" + 0.002*\"see\" + 0.002*\"little\"\n",
      "2017-11-02 19:06:41,787 : INFO : topic #9 (0.100): 0.003*\"said\" + 0.003*\"one\" + 0.003*\"like\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"would\" + 0.002*\"know\" + 0.002*\"thought\" + 0.002*\"right\" + 0.001*\"looked\"\n",
      "2017-11-02 19:06:41,840 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"like\" + 0.003*\"one\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"time\" + 0.002*\"got\"\n",
      "2017-11-02 19:06:41,893 : INFO : topic #4 (0.100): 0.005*\"said\" + 0.004*\"like\" + 0.003*\"one\" + 0.003*\"would\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"little\" + 0.001*\"looked\" + 0.001*\"get\" + 0.001*\"even\"\n",
      "2017-11-02 19:06:42,029 : INFO : topic diff=3.440557, rho=1.000000\n",
      "2017-11-02 19:09:26,493 : INFO : -13.682 per-word bound, 13141.3 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:09:26,494 : INFO : PROGRESS: pass 1, at document #67/67\n",
      "2017-11-02 19:09:42,549 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"right\" + 0.002*\"know\" + 0.002*\"little\"\n",
      "2017-11-02 19:09:42,596 : INFO : topic #7 (0.100): 0.003*\"said\" + 0.003*\"one\" + 0.002*\"back\" + 0.002*\"would\" + 0.001*\"thought\" + 0.001*\"could\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"right\" + 0.001*\"see\"\n",
      "2017-11-02 19:09:42,656 : INFO : topic #9 (0.100): 0.002*\"said\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"would\" + 0.001*\"know\" + 0.001*\"thought\" + 0.001*\"right\" + 0.001*\"looked\"\n",
      "2017-11-02 19:09:42,692 : INFO : topic #6 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"thought\" + 0.002*\"little\" + 0.002*\"know\"\n",
      "2017-11-02 19:09:42,741 : INFO : topic #4 (0.100): 0.004*\"said\" + 0.004*\"like\" + 0.003*\"one\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"could\" + 0.001*\"little\" + 0.001*\"looked\" + 0.001*\"get\" + 0.001*\"time\"\n",
      "2017-11-02 19:09:42,865 : INFO : topic diff=1.682576, rho=0.577350\n",
      "2017-11-02 19:12:26,366 : INFO : -12.749 per-word bound, 6884.5 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:12:26,368 : INFO : PROGRESS: pass 2, at document #67/67\n",
      "2017-11-02 19:12:42,705 : INFO : topic #0 (0.100): 0.004*\"said\" + 0.003*\"one\" + 0.003*\"roland\" + 0.003*\"would\" + 0.003*\"like\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"looked\" + 0.002*\"thought\"\n",
      "2017-11-02 19:12:42,762 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"right\" + 0.002*\"know\" + 0.002*\"little\"\n",
      "2017-11-02 19:12:42,819 : INFO : topic #9 (0.100): 0.002*\"said\" + 0.001*\"one\" + 0.001*\"like\" + 0.001*\"back\" + 0.001*\"could\" + 0.001*\"would\" + 0.001*\"know\" + 0.001*\"thought\" + 0.001*\"right\" + 0.001*\"looked\"\n",
      "2017-11-02 19:12:42,864 : INFO : topic #4 (0.100): 0.004*\"said\" + 0.003*\"like\" + 0.002*\"one\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"could\" + 0.001*\"danny\" + 0.001*\"little\" + 0.001*\"looked\" + 0.001*\"get\"\n",
      "2017-11-02 19:12:42,897 : INFO : topic #6 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"thought\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"man\"\n",
      "2017-11-02 19:12:43,033 : INFO : topic diff=1.089075, rho=0.500000\n",
      "2017-11-02 19:15:28,331 : INFO : -12.462 per-word bound, 5642.3 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:15:28,350 : INFO : PROGRESS: pass 3, at document #67/67\n",
      "2017-11-02 19:16:06,296 : INFO : topic #4 (0.100): 0.003*\"said\" + 0.003*\"like\" + 0.002*\"one\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"could\" + 0.001*\"danny\" + 0.001*\"jack\" + 0.001*\"time\" + 0.001*\"get\"\n",
      "2017-11-02 19:16:06,340 : INFO : topic #0 (0.100): 0.004*\"said\" + 0.004*\"roland\" + 0.003*\"one\" + 0.003*\"would\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"looked\" + 0.002*\"eddie\"\n",
      "2017-11-02 19:16:06,397 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"right\" + 0.002*\"little\"\n",
      "2017-11-02 19:16:06,452 : INFO : topic #2 (0.100): 0.003*\"said\" + 0.003*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.001*\"dan\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"little\"\n",
      "2017-11-02 19:16:06,510 : INFO : topic #3 (0.100): 0.002*\"said\" + 0.002*\"one\" + 0.001*\"like\" + 0.001*\"back\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"would\" + 0.001*\"think\" + 0.001*\"thought\" + 0.001*\"man\"\n",
      "2017-11-02 19:16:06,649 : INFO : topic diff=0.686293, rho=0.447214\n",
      "2017-11-02 19:19:01,556 : INFO : -12.356 per-word bound, 5242.0 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:19:01,558 : INFO : PROGRESS: pass 4, at document #67/67\n",
      "2017-11-02 19:19:46,159 : INFO : topic #7 (0.100): 0.001*\"said\" + 0.001*\"one\" + 0.001*\"back\" + 0.001*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"\n",
      "2017-11-02 19:19:46,203 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"right\" + 0.002*\"little\"\n",
      "2017-11-02 19:19:46,258 : INFO : topic #2 (0.100): 0.003*\"said\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"dan\" + 0.002*\"would\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"abra\" + 0.001*\"time\"\n",
      "2017-11-02 19:19:46,302 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"\n",
      "2017-11-02 19:19:46,347 : INFO : topic #4 (0.100): 0.003*\"said\" + 0.002*\"like\" + 0.002*\"one\" + 0.002*\"would\" + 0.002*\"back\" + 0.002*\"danny\" + 0.001*\"could\" + 0.001*\"jack\" + 0.001*\"time\" + 0.001*\"get\"\n",
      "2017-11-02 19:19:46,460 : INFO : topic diff=0.439985, rho=0.408248\n",
      "2017-11-02 19:22:37,314 : INFO : -12.307 per-word bound, 5067.8 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:22:37,316 : INFO : PROGRESS: pass 5, at document #67/67\n",
      "2017-11-02 19:23:19,804 : INFO : topic #2 (0.100): 0.003*\"said\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"dan\" + 0.002*\"back\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"abra\" + 0.001*\"know\" + 0.001*\"little\"\n",
      "2017-11-02 19:23:19,846 : INFO : topic #4 (0.100): 0.003*\"said\" + 0.002*\"like\" + 0.002*\"one\" + 0.002*\"danny\" + 0.002*\"would\" + 0.002*\"back\" + 0.001*\"could\" + 0.001*\"jack\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2017-11-02 19:23:19,886 : INFO : topic #7 (0.100): 0.001*\"said\" + 0.001*\"one\" + 0.000*\"back\" + 0.000*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"\n",
      "2017-11-02 19:23:19,937 : INFO : topic #3 (0.100): 0.001*\"said\" + 0.001*\"one\" + 0.001*\"wesley\" + 0.001*\"like\" + 0.000*\"back\" + 0.000*\"know\" + 0.000*\"could\" + 0.000*\"would\" + 0.000*\"think\" + 0.000*\"thought\"\n",
      "2017-11-02 19:23:19,979 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"right\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 19:23:20,101 : INFO : topic diff=0.286577, rho=0.377964\n",
      "2017-11-02 19:26:11,274 : INFO : -12.282 per-word bound, 4981.2 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:26:11,276 : INFO : PROGRESS: pass 6, at document #67/67\n",
      "2017-11-02 19:26:53,785 : INFO : topic #9 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"would\" + 0.000*\"know\" + 0.000*\"thought\" + 0.000*\"right\" + 0.000*\"looked\"\n",
      "2017-11-02 19:26:53,833 : INFO : topic #2 (0.100): 0.003*\"said\" + 0.002*\"one\" + 0.002*\"dan\" + 0.002*\"like\" + 0.002*\"back\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"abra\" + 0.001*\"know\" + 0.001*\"little\"\n",
      "2017-11-02 19:26:53,871 : INFO : topic #7 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"back\" + 0.000*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"\n",
      "2017-11-02 19:26:53,916 : INFO : topic #3 (0.100): 0.001*\"said\" + 0.001*\"wesley\" + 0.001*\"one\" + 0.000*\"like\" + 0.000*\"back\" + 0.000*\"kindle\" + 0.000*\"robbie\" + 0.000*\"know\" + 0.000*\"could\" + 0.000*\"would\"\n",
      "2017-11-02 19:26:53,944 : INFO : topic #1 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"time\"\n",
      "2017-11-02 19:26:54,059 : INFO : topic diff=0.189872, rho=0.353553\n",
      "2017-11-02 19:29:47,655 : INFO : -12.269 per-word bound, 4934.5 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:29:47,656 : INFO : PROGRESS: pass 7, at document #67/67\n",
      "2017-11-02 19:30:31,350 : INFO : topic #3 (0.100): 0.001*\"wesley\" + 0.001*\"said\" + 0.000*\"one\" + 0.000*\"kindle\" + 0.000*\"robbie\" + 0.000*\"like\" + 0.000*\"ur\" + 0.000*\"back\" + 0.000*\"know\" + 0.000*\"could\"\n",
      "2017-11-02 19:30:31,393 : INFO : topic #0 (0.100): 0.004*\"roland\" + 0.004*\"said\" + 0.004*\"one\" + 0.003*\"would\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"could\" + 0.002*\"eddie\" + 0.002*\"susannah\" + 0.002*\"jake\"\n",
      "2017-11-02 19:30:31,440 : INFO : topic #2 (0.100): 0.002*\"said\" + 0.002*\"one\" + 0.002*\"dan\" + 0.002*\"like\" + 0.002*\"back\" + 0.001*\"would\" + 0.001*\"abra\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"little\"\n",
      "2017-11-02 19:30:31,483 : INFO : topic #4 (0.100): 0.003*\"said\" + 0.002*\"one\" + 0.002*\"like\" + 0.002*\"danny\" + 0.002*\"back\" + 0.002*\"would\" + 0.001*\"jack\" + 0.001*\"could\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2017-11-02 19:30:31,530 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"\n",
      "2017-11-02 19:30:31,654 : INFO : topic diff=0.127954, rho=0.333333\n",
      "2017-11-02 19:33:24,438 : INFO : -12.261 per-word bound, 4907.4 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:33:24,440 : INFO : PROGRESS: pass 8, at document #67/67\n",
      "2017-11-02 19:34:06,793 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"\n",
      "2017-11-02 19:34:06,842 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"right\"\n",
      "2017-11-02 19:34:06,873 : INFO : topic #1 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"time\"\n",
      "2017-11-02 19:34:06,914 : INFO : topic #0 (0.100): 0.004*\"roland\" + 0.004*\"said\" + 0.004*\"one\" + 0.003*\"would\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"eddie\" + 0.002*\"could\" + 0.002*\"susannah\" + 0.002*\"jake\"\n",
      "2017-11-02 19:34:06,961 : INFO : topic #3 (0.100): 0.001*\"wesley\" + 0.000*\"said\" + 0.000*\"kindle\" + 0.000*\"robbie\" + 0.000*\"one\" + 0.000*\"ur\" + 0.000*\"like\" + 0.000*\"ellen\" + 0.000*\"back\" + 0.000*\"know\"\n",
      "2017-11-02 19:34:07,076 : INFO : topic diff=0.087617, rho=0.316228\n",
      "2017-11-02 19:36:57,146 : INFO : -12.256 per-word bound, 4890.9 perplexity estimate based on a held-out corpus of 67 documents with 10236337 words\n",
      "2017-11-02 19:36:57,148 : INFO : PROGRESS: pass 9, at document #67/67\n",
      "2017-11-02 19:37:39,408 : INFO : topic #9 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"would\" + 0.000*\"know\" + 0.000*\"thought\" + 0.000*\"right\" + 0.000*\"looked\"\n",
      "2017-11-02 19:37:39,441 : INFO : topic #6 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"thought\" + 0.002*\"time\" + 0.002*\"man\"\n",
      "2017-11-02 19:37:39,488 : INFO : topic #2 (0.100): 0.002*\"said\" + 0.002*\"dan\" + 0.002*\"one\" + 0.002*\"like\" + 0.001*\"back\" + 0.001*\"abra\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"little\"\n",
      "2017-11-02 19:37:39,525 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"\n",
      "2017-11-02 19:37:39,567 : INFO : topic #7 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"back\" + 0.000*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"\n",
      "2017-11-02 19:37:39,690 : INFO : topic diff=0.060899, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "# lda2 = models.LdaModel(corpus=corp2, num_topics=10, id2word=id2word2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T23:37:40.429516Z",
     "start_time": "2017-11-02T23:37:39.921916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-02 19:37:40,060 : INFO : topic #0 (0.100): 0.005*\"roland\" + 0.004*\"said\" + 0.004*\"one\" + 0.003*\"would\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"eddie\" + 0.002*\"could\" + 0.002*\"susannah\" + 0.002*\"jake\"\n",
      "2017-11-02 19:37:40,086 : INFO : topic #1 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"time\"\n",
      "2017-11-02 19:37:40,132 : INFO : topic #2 (0.100): 0.002*\"said\" + 0.002*\"dan\" + 0.002*\"one\" + 0.002*\"like\" + 0.001*\"back\" + 0.001*\"abra\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"little\"\n",
      "2017-11-02 19:37:40,180 : INFO : topic #3 (0.100): 0.001*\"wesley\" + 0.000*\"kindle\" + 0.000*\"robbie\" + 0.000*\"said\" + 0.000*\"ur\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"ellen\" + 0.000*\"back\" + 0.000*\"know\"\n",
      "2017-11-02 19:37:40,220 : INFO : topic #4 (0.100): 0.003*\"said\" + 0.002*\"one\" + 0.002*\"danny\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"jack\" + 0.001*\"could\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2017-11-02 19:37:40,267 : INFO : topic #5 (0.100): 0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"right\"\n",
      "2017-11-02 19:37:40,295 : INFO : topic #6 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"thought\" + 0.002*\"time\" + 0.002*\"man\"\n",
      "2017-11-02 19:37:40,336 : INFO : topic #7 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"back\" + 0.000*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"\n",
      "2017-11-02 19:37:40,371 : INFO : topic #8 (0.100): 0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"\n",
      "2017-11-02 19:37:40,415 : INFO : topic #9 (0.100): 0.000*\"said\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"would\" + 0.000*\"know\" + 0.000*\"thought\" + 0.000*\"right\" + 0.000*\"looked\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"roland\" + 0.004*\"said\" + 0.004*\"one\" + 0.003*\"would\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"eddie\" + 0.002*\"could\" + 0.002*\"susannah\" + 0.002*\"jake\"'),\n",
       " (1,\n",
       "  '0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"time\"'),\n",
       " (2,\n",
       "  '0.002*\"said\" + 0.002*\"dan\" + 0.002*\"one\" + 0.002*\"like\" + 0.001*\"back\" + 0.001*\"abra\" + 0.001*\"would\" + 0.001*\"could\" + 0.001*\"know\" + 0.001*\"little\"'),\n",
       " (3,\n",
       "  '0.001*\"wesley\" + 0.000*\"kindle\" + 0.000*\"robbie\" + 0.000*\"said\" + 0.000*\"ur\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"ellen\" + 0.000*\"back\" + 0.000*\"know\"'),\n",
       " (4,\n",
       "  '0.003*\"said\" + 0.002*\"one\" + 0.002*\"danny\" + 0.002*\"like\" + 0.002*\"back\" + 0.002*\"would\" + 0.002*\"jack\" + 0.001*\"could\" + 0.001*\"time\" + 0.001*\"know\"'),\n",
       " (5,\n",
       "  '0.004*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"back\" + 0.002*\"would\" + 0.002*\"could\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"little\" + 0.002*\"right\"'),\n",
       " (6,\n",
       "  '0.005*\"said\" + 0.004*\"one\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"back\" + 0.002*\"could\" + 0.002*\"know\" + 0.002*\"thought\" + 0.002*\"time\" + 0.002*\"man\"'),\n",
       " (7,\n",
       "  '0.000*\"said\" + 0.000*\"one\" + 0.000*\"back\" + 0.000*\"would\" + 0.000*\"thought\" + 0.000*\"could\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"right\" + 0.000*\"see\"'),\n",
       " (8,\n",
       "  '0.005*\"said\" + 0.004*\"one\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"would\" + 0.003*\"could\" + 0.002*\"time\" + 0.002*\"little\" + 0.002*\"looked\" + 0.002*\"know\"'),\n",
       " (9,\n",
       "  '0.000*\"said\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"back\" + 0.000*\"could\" + 0.000*\"would\" + 0.000*\"know\" + 0.000*\"thought\" + 0.000*\"right\" + 0.000*\"looked\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T18:28:38.767770Z",
     "start_time": "2017-11-03T18:28:38.762790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shan', 'too', 'wasn', 'under', 'about', 'than', 'can', 'further', 'above', 'o', 't', 'herself', 'again', 'at', 'was', 'didn', 'we', 'isn', 'does', 'me', 'i', 'couldn', 'the', 'been', 'she', 'her', \"'\", 'as', 'those', 'then', 'until', 'these', 'when', 'both', 'y', 'before', 'they', 'same', 're', ')', 'each', 'were', 'are', 'mightn', 'mustn', 'nor', 'very', 'of', 'how', 'don', 'any', 'ours', 'your', 'there', 'our', 'm', 'being', 'aren', 'do', 'so', 'am', 'some', 'such', '.', 'did', 'up', 'through', 'not', 'myself', 'you', 'should', 'between', 'out', 'where', 'to', 'weren', 'for', 's', 'haven', 'yourself', 'whom', 'down', '(', 'from', 'my', 'yours', 'ain', 'is', 'yourselves', 'their', 'which', 'what', 've', 'had', 'needn', 'himself', 'more', 'an', 'few', 'here', 'a', 'but', 'below', 'if', '\"', 'why', 'doing', 'all', 'on', 'itself', 'just', 'hadn', 'them', 'own', 'hasn', 'only', 'will', 'wouldn', 'be', 'him', ',', 'shouldn', 'by', 'against', 'with', 'it', 'into', 'he', 'theirs', 'ourselves', 'd', 'and', 'now', 'll', 'no', 'most', 'having', 'because', 'once', 'themselves', 'off', 'his', 'in', 'other', 'while', 'during', 'ma', 'this', 'hers', 'doesn', 'over', 'won', 'or', 'has', 'after', 'who', 'that', 'have', 'its'}\n"
     ]
    }
   ],
   "source": [
    "# print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Splitting the text for just the story content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T20:37:19.451144Z",
     "start_time": "2017-11-01T20:37:19.447273Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start = gs_text.find('******start_of_file******')+25\n",
    "# end = gs_text.find('******end_of_file******')\n",
    "# gs_text = gs_text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T20:38:21.803187Z",
     "start_time": "2017-11-01T20:38:21.798369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ble final battle.\\n\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gs_text[-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T19:42:12.701995Z",
     "start_time": "2017-11-04T19:42:12.694825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words = [''.join(words) for words in gs_text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T19:04:35.541898Z",
     "start_time": "2017-11-02T19:04:35.494259Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "n = 1\n",
    "words = [w for w in words if w not in stoplist]\n",
    "bigrams = ngrams(words, n)\n",
    "counter += Counter(bigrams)\n",
    "\n",
    "sorted_counter = sorted(counter.items(), key=operator.itemgetter(1),reverse=True)\n",
    "# for word, count in gBlob.word_counts.items():\n",
    "#     print(\"%15s %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T19:04:36.793976Z",
     "start_time": "2017-11-02T19:04:36.743195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('gunslinger',), 531),\n",
       " (('man',), 243),\n",
       " (('boy',), 236),\n",
       " (('one',), 218),\n",
       " (('like',), 201),\n",
       " (('would',), 186),\n",
       " (('“i',), 169),\n",
       " (('black',), 156),\n",
       " (('said.',), 151),\n",
       " (('looked',), 147),\n",
       " (('back',), 143),\n",
       " (('could',), 136),\n",
       " (('said',), 129),\n",
       " (('jake',), 120),\n",
       " (('made',), 103),\n",
       " (('don’t',), 99),\n",
       " (('even',), 92),\n",
       " (('him.',), 92),\n",
       " (('it.',), 90),\n",
       " (('him,',), 89),\n",
       " (('eyes',), 88),\n",
       " (('seemed',), 87),\n",
       " (('came',), 85),\n",
       " (('roland',), 85),\n",
       " (('face',), 83),\n",
       " (('went',), 82),\n",
       " (('felt',), 81),\n",
       " (('might',), 80),\n",
       " (('way',), 77),\n",
       " (('still',), 77),\n",
       " (('know',), 76),\n",
       " (('time',), 75),\n",
       " (('never',), 74),\n",
       " (('come',), 74),\n",
       " (('“you',), 73),\n",
       " (('see',), 71),\n",
       " (('first',), 69),\n",
       " (('two',), 69),\n",
       " (('thought',), 68),\n",
       " (('cort',), 68),\n",
       " (('began',), 66),\n",
       " (('hands',), 66),\n",
       " (('said,',), 66),\n",
       " (('long',), 65),\n",
       " (('it,',), 65),\n",
       " (('last',), 65),\n",
       " (('go',), 64),\n",
       " (('almost',), 63),\n",
       " (('head',), 60),\n",
       " (('didn’t',), 59),\n",
       " (('saw',), 59),\n",
       " (('perhaps',), 59),\n",
       " (('old',), 58),\n",
       " (('three',), 57),\n",
       " (('think',), 57),\n",
       " (('“the',), 57),\n",
       " (('little',), 55),\n",
       " (('left',), 55),\n",
       " (('light',), 54),\n",
       " (('stood',), 54),\n",
       " (('turned',), 53),\n",
       " (('sound',), 53),\n",
       " (('voice',), 53),\n",
       " (('hand',), 53),\n",
       " (('put',), 52),\n",
       " (('want',), 52),\n",
       " (('around',), 50),\n",
       " (('knew',), 50),\n",
       " (('held',), 49),\n",
       " (('away',), 49),\n",
       " (('brown',), 47),\n",
       " (('dark',), 46),\n",
       " (('something',), 46),\n",
       " (('cuthbert',), 46),\n",
       " (('great',), 45),\n",
       " (('got',), 45),\n",
       " (('toward',), 45),\n",
       " (('tell',), 44),\n",
       " (('let',), 44),\n",
       " (('going',), 44),\n",
       " (('father',), 44),\n",
       " (('get',), 43),\n",
       " (('it’s',), 43),\n",
       " (('yet',), 43),\n",
       " (('moment',), 43),\n",
       " (('walked',), 42),\n",
       " (('far',), 42),\n",
       " (('again.',), 42),\n",
       " (('across',), 42),\n",
       " (('always',), 41),\n",
       " (('again,',), 41),\n",
       " (('boy’s',), 40),\n",
       " (('make',), 39),\n",
       " (('moved',), 39),\n",
       " (('world',), 38),\n",
       " (('although',), 38),\n",
       " (('say',), 38),\n",
       " (('nothing',), 38),\n",
       " (('much',), 38),\n",
       " (('them,',), 38),\n",
       " (('gone',), 38),\n",
       " (('fell',), 38),\n",
       " (('took',), 37),\n",
       " (('looking',), 37),\n",
       " (('them.',), 37),\n",
       " (('sat',), 37),\n",
       " (('enough',), 36),\n",
       " (('right',), 35),\n",
       " (('good',), 35),\n",
       " (('seen',), 35),\n",
       " (('gunslinger’s',), 35),\n",
       " (('beyond',), 35),\n",
       " (('feel',), 34),\n",
       " (('mind',), 34),\n",
       " (('behind',), 34),\n",
       " (('told',), 33),\n",
       " (('end',), 33),\n",
       " (('must',), 33),\n",
       " (('water',), 33),\n",
       " (('i’m',), 32),\n",
       " (('heard',), 32),\n",
       " (('now,',), 32),\n",
       " (('now.',), 32),\n",
       " (('slow',), 31),\n",
       " (('passed',), 31),\n",
       " (('look',), 31),\n",
       " (('you,',), 31),\n",
       " (('another',), 31),\n",
       " (('found',), 31),\n",
       " (('watched',), 31),\n",
       " (('up,',), 31),\n",
       " (('years',), 30),\n",
       " (('day',), 30),\n",
       " (('beneath',), 30),\n",
       " (('smell',), 30),\n",
       " (('dead',), 29),\n",
       " (('side',), 29),\n",
       " (('“what',), 29),\n",
       " (('may',), 28),\n",
       " (('without',), 28),\n",
       " (('body',), 28),\n",
       " (('back.',), 28),\n",
       " (('desert',), 28),\n",
       " (('ever',), 28),\n",
       " (('struck',), 28),\n",
       " (('coming',), 28),\n",
       " (('wind',), 28),\n",
       " (('place',), 28),\n",
       " (('asked.',), 28),\n",
       " (('“he',), 28),\n",
       " (('.”',), 28),\n",
       " (('boys',), 28),\n",
       " (('huge',), 28),\n",
       " (('blood',), 28),\n",
       " (('tower',), 27),\n",
       " (('green',), 27),\n",
       " (('wanted',), 27),\n",
       " (('face.',), 27),\n",
       " (('called',), 27),\n",
       " (('mouth',), 27),\n",
       " (('white',), 27),\n",
       " (('small',), 27),\n",
       " (('asked',), 27),\n",
       " (('suddenly',), 27),\n",
       " (('caught',), 27),\n",
       " (('hawk',), 27),\n",
       " (('feet',), 26),\n",
       " (('high',), 26),\n",
       " (('door',), 26),\n",
       " (('“it',), 26),\n",
       " (('cort’s',), 26),\n",
       " (('that,',), 25),\n",
       " (('mother',), 25),\n",
       " (('every',), 25),\n",
       " (('find',), 25),\n",
       " (('up.',), 25),\n",
       " (('sun',), 25),\n",
       " (('head.',), 25),\n",
       " (('“i’m',), 25),\n",
       " (('“but',), 25),\n",
       " (('new',), 24),\n",
       " (('things',), 24),\n",
       " (('four',), 24),\n",
       " (('eye',), 24),\n",
       " (('take',), 24),\n",
       " (('gunslinger,',), 24),\n",
       " (('on.',), 24),\n",
       " (('reached',), 24),\n",
       " (('also',), 23),\n",
       " (('night',), 23),\n",
       " (('sense',), 23),\n",
       " (('feeling',), 23),\n",
       " (('since',), 23),\n",
       " (('later',), 23),\n",
       " (('open',), 23),\n",
       " (('rising',), 23),\n",
       " (('fire',), 23),\n",
       " (('lay',), 23),\n",
       " (('shook',), 23),\n",
       " (('he’d',), 23),\n",
       " (('kill',), 23),\n",
       " (('become',), 23),\n",
       " (('woman',), 23),\n",
       " (('her.',), 23),\n",
       " (('stone',), 23),\n",
       " (('many',), 22),\n",
       " (('love',), 22),\n",
       " (('need',), 22),\n",
       " (('tiny',), 22),\n",
       " (('spoke',), 22),\n",
       " (('“and',), 22),\n",
       " (('“it’s',), 22),\n",
       " (('face,',), 22),\n",
       " (('allie',), 22),\n",
       " (('probably',), 21),\n",
       " (('me,',), 21),\n",
       " (('maybe',), 21),\n",
       " (('time.',), 21),\n",
       " (('thing',), 21),\n",
       " (('us',), 21),\n",
       " (('beside',), 21),\n",
       " (('turn',), 21),\n",
       " (('words',), 21),\n",
       " (('i’ll',), 21),\n",
       " (('close',), 21),\n",
       " (('nodded',), 21),\n",
       " (('gunslinger.',), 21),\n",
       " (('dropped',), 21),\n",
       " (('hear',), 21),\n",
       " (('boy,',), 21),\n",
       " (('kennerly',), 21),\n",
       " (('rolled',), 21),\n",
       " (('jake’s',), 21),\n",
       " (('rock',), 20),\n",
       " (('you.',), 20),\n",
       " (('watch',), 20),\n",
       " (('large',), 20),\n",
       " (('really',), 20),\n",
       " (('out.',), 20),\n",
       " (('gave',), 20),\n",
       " (('sky',), 20),\n",
       " (('already',), 20),\n",
       " (('blue',), 20),\n",
       " (('drew',), 20),\n",
       " (('eyes,',), 20),\n",
       " (('speak',), 20),\n",
       " (('half',), 19),\n",
       " (('gray',), 19),\n",
       " (('next',), 19),\n",
       " (('can’t',), 19),\n",
       " (('young',), 19),\n",
       " (('lost',), 19),\n",
       " (('remember',), 19),\n",
       " (('guns',), 19),\n",
       " (('light.',), 19),\n",
       " (('shadows',), 19),\n",
       " (('sometimes',), 19),\n",
       " (('“yes.”',), 19),\n",
       " (('eyes.',), 19),\n",
       " (('hard',), 19),\n",
       " (('“all',), 19),\n",
       " (('“do',), 19),\n",
       " (('sleep',), 19),\n",
       " (('time,',), 19),\n",
       " (('fingers',), 19),\n",
       " (('darkness',), 19),\n",
       " (('hair',), 18),\n",
       " (('you’re',), 18),\n",
       " (('able',), 18),\n",
       " (('pushed',), 18),\n",
       " (('final',), 18),\n",
       " (('people',), 18),\n",
       " (('someone',), 18),\n",
       " (('hold',), 18),\n",
       " (('wasn’t',), 18),\n",
       " (('god',), 18),\n",
       " (('followed',), 18),\n",
       " (('threw',), 18),\n",
       " (('know.',), 18),\n",
       " (('“a',), 18),\n",
       " (('stepped',), 18),\n",
       " (('shot',), 18),\n",
       " (('smiled',), 18),\n",
       " (('you.”',), 18),\n",
       " (('arms',), 18),\n",
       " (('nort',), 18),\n",
       " (('arm',), 18),\n",
       " (('stand',), 17),\n",
       " (('full',), 17),\n",
       " (('set',), 17),\n",
       " (('bad',), 17),\n",
       " (('hand.',), 17),\n",
       " (('me.',), 17),\n",
       " (('away,',), 17),\n",
       " (('word',), 17),\n",
       " (('flat',), 17),\n",
       " (('“that’s',), 17),\n",
       " (('hung',), 17),\n",
       " (('it.”',), 17),\n",
       " (('thought.',), 17),\n",
       " (('touched',), 17),\n",
       " (('screamed',), 17),\n",
       " (('none',), 17),\n",
       " (('hax',), 17),\n",
       " (('red',), 16),\n",
       " (('mountains',), 16),\n",
       " (('five',), 16),\n",
       " (('kind',), 16),\n",
       " (('started',), 16),\n",
       " (('idea',), 16),\n",
       " (('ran',), 16),\n",
       " (('somewhere',), 16),\n",
       " (('run',), 16),\n",
       " (('roland’s',), 16),\n",
       " (('given',), 16),\n",
       " (('strange',), 16),\n",
       " (('flew',), 16),\n",
       " (('“i’ll',), 16),\n",
       " (('away.',), 16),\n",
       " (('closed',), 16),\n",
       " (('won’t',), 16),\n",
       " (('sheb',), 16),\n",
       " (('heavy',), 16),\n",
       " (('filled',), 16),\n",
       " (('handcar',), 16),\n",
       " (('whole',), 15),\n",
       " (('read',), 15),\n",
       " (('men',), 15),\n",
       " (('age',), 15),\n",
       " (('way,',), 15),\n",
       " (('smoke',), 15),\n",
       " (('ask',), 15),\n",
       " (('part',), 15),\n",
       " (('“not',), 15),\n",
       " (('all,',), 15),\n",
       " (('this,',), 15),\n",
       " (('lips',), 15),\n",
       " (('tracks',), 15),\n",
       " (('wondered',), 15),\n",
       " (('others',), 15),\n",
       " (('corn',), 15),\n",
       " (('name',), 15),\n",
       " (('you?”',), 15),\n",
       " (('“how',), 15),\n",
       " (('nodded.',), 15),\n",
       " (('catch',), 15),\n",
       " (('fire.',), 15),\n",
       " (('black.',), 15),\n",
       " (('onto',), 15),\n",
       " (('stared',), 15),\n",
       " (('faces',), 15),\n",
       " (('led',), 15),\n",
       " (('front',), 15),\n",
       " (('“go',), 15),\n",
       " (('air.',), 15),\n",
       " (('fear',), 15),\n",
       " (('stopped',), 15),\n",
       " (('silent',), 15),\n",
       " (('making',), 14),\n",
       " (('rose',), 14),\n",
       " (('girl',), 14),\n",
       " (('mutants',), 14),\n",
       " (('paused',), 14),\n",
       " (('big',), 14),\n",
       " (('rather',), 14),\n",
       " (('then,',), 14),\n",
       " (('story',), 14),\n",
       " (('comes',), 14),\n",
       " (('carried',), 14),\n",
       " (('smile',), 14),\n",
       " (('done',), 14),\n",
       " (('known',), 14),\n",
       " (('upon',), 14),\n",
       " (('deep',), 14),\n",
       " (('seem',), 14),\n",
       " (('way.',), 14),\n",
       " (('better',), 14),\n",
       " (('there.',), 14),\n",
       " (('brought',), 14),\n",
       " (('yellow',), 14),\n",
       " (('light,',), 14),\n",
       " (('cold',), 14),\n",
       " (('thin',), 14),\n",
       " (('together',), 14),\n",
       " (('mule',), 14),\n",
       " (('ain’t',), 14),\n",
       " (('sudden',), 14),\n",
       " (('turning',), 14),\n",
       " (('dry',), 14),\n",
       " (('hot',), 14),\n",
       " (('“if',), 14),\n",
       " (('nearly',), 14),\n",
       " (('taken',), 14),\n",
       " (('back,',), 14),\n",
       " (('became',), 14),\n",
       " (('holding',), 14),\n",
       " (('pulled',), 14),\n",
       " (('land',), 14),\n",
       " (('air',), 14),\n",
       " (('boy.',), 14),\n",
       " (('marten',), 14),\n",
       " (('either',), 13),\n",
       " (('used',), 13),\n",
       " (('book',), 13),\n",
       " (('writing',), 13),\n",
       " (('(the',), 13),\n",
       " (('sure',), 13),\n",
       " (('begun',), 13),\n",
       " (('that’s',), 13),\n",
       " (('heart',), 13),\n",
       " (('he’s',), 13),\n",
       " (('start',), 13),\n",
       " (('empty',), 13),\n",
       " (('understand',), 13),\n",
       " (('single',), 13),\n",
       " (('line',), 13),\n",
       " (('quite',), 13),\n",
       " (('along',), 13),\n",
       " (('longer',), 13),\n",
       " (('flesh',), 13),\n",
       " (('work',), 13),\n",
       " (('hands.',), 13),\n",
       " (('beginning',), 13),\n",
       " (('right.',), 13),\n",
       " (('give',), 13),\n",
       " (('cut',), 13),\n",
       " (('father’s',), 13),\n",
       " (('huge,',), 13),\n",
       " (('raised',), 13),\n",
       " (('grass',), 13),\n",
       " (('desert.',), 13),\n",
       " (('nothing.',), 13),\n",
       " (('laid',), 13),\n",
       " (('drawn',), 13),\n",
       " (('watching',), 13),\n",
       " (('“then',), 13),\n",
       " (('bird',), 13),\n",
       " (('room',), 13),\n",
       " (('piano',), 13),\n",
       " (('“there',), 13),\n",
       " (('“no',), 13),\n",
       " (('children',), 13),\n",
       " (('water.',), 13),\n",
       " (('there,',), 13),\n",
       " (('more.',), 13),\n",
       " (('shirt',), 13),\n",
       " (('legs',), 13),\n",
       " (('faint',), 13),\n",
       " (('living',), 12),\n",
       " (('walk',), 12),\n",
       " (('(and',), 12),\n",
       " (('least',), 12),\n",
       " (('write',), 12),\n",
       " (('out,',), 12),\n",
       " (('change',), 12),\n",
       " (('fine',), 12),\n",
       " (('fall',), 12),\n",
       " (('whose',), 12),\n",
       " (('side,',), 12),\n",
       " (('trying',), 12),\n",
       " (('clear',), 12),\n",
       " (('man,',), 12),\n",
       " (('point',), 12),\n",
       " (('sand',), 12),\n",
       " (('ate',), 12),\n",
       " (('ancient',), 12),\n",
       " (('moving',), 12),\n",
       " (('steel',), 12),\n",
       " (('town',), 12),\n",
       " (('dweller',), 12),\n",
       " (('afraid',), 12),\n",
       " (('me.”',), 12),\n",
       " (('it?”',), 12),\n",
       " (('other,',), 12),\n",
       " (('earth',), 12),\n",
       " (('touch',), 12),\n",
       " (('killed',), 12),\n",
       " (('“will',), 12),\n",
       " (('slowly',), 12),\n",
       " (('crossed',), 12),\n",
       " (('continued',), 12),\n",
       " (('center',), 12),\n",
       " (('“are',), 12),\n",
       " (('ones',), 12),\n",
       " (('david',), 12),\n",
       " (('cry',), 12),\n",
       " (('thought,',), 12),\n",
       " (('lot',), 11),\n",
       " (('one.',), 11),\n",
       " (('wouldn’t',), 11),\n",
       " (('anything',), 11),\n",
       " (('else',), 11),\n",
       " (('here.',), 11),\n",
       " (('(or',), 11),\n",
       " (('mean',), 11),\n",
       " (('yes,',), 11),\n",
       " (('sort',), 11),\n",
       " (('believe',), 11),\n",
       " (('tried',), 11),\n",
       " (('less',), 11),\n",
       " (('near',), 11),\n",
       " (('“why',), 11),\n",
       " (('urge',), 11),\n",
       " (('thinking',), 11),\n",
       " (('edge',), 11),\n",
       " (('doesn’t',), 11),\n",
       " (('man’s',), 11),\n",
       " (('here,',), 11),\n",
       " (('thick',), 11),\n",
       " (('rain',), 11),\n",
       " (('grin',), 11),\n",
       " (('burned',), 11),\n",
       " (('true',), 11),\n",
       " (('stars',), 11),\n",
       " (('do.”',), 11),\n",
       " (('that?”',), 11),\n",
       " (('rabbit',), 11),\n",
       " (('rest',), 11),\n",
       " (('cigarette',), 11),\n",
       " (('move',), 11),\n",
       " (('wearing',), 11),\n",
       " (('shadow',), 11),\n",
       " (('gold',), 11),\n",
       " (('bright',), 11),\n",
       " (('smile.',), 11),\n",
       " (('knife',), 11),\n",
       " (('“who',), 11),\n",
       " (('wet',), 11),\n",
       " (('not.',), 11),\n",
       " (('faded',), 11),\n",
       " (('trap',), 11),\n",
       " (('“when',), 11),\n",
       " (('feet.',), 11),\n",
       " (('sylvia',), 11),\n",
       " (('know.”',), 11),\n",
       " (('“we',), 11),\n",
       " (('cort,',), 11),\n",
       " (('climbed',), 11),\n",
       " (('king',), 10),\n",
       " (('wide',), 10),\n",
       " (('past',), 10),\n",
       " (('chapter',), 10),\n",
       " (('station',), 10),\n",
       " (('oracle',), 10),\n",
       " (('twice',), 10),\n",
       " (('nineteen,',), 10),\n",
       " (('that.',), 10),\n",
       " (('show',), 10),\n",
       " (('isn’t',), 10),\n",
       " (('down,',), 10),\n",
       " (('card',), 10),\n",
       " (('ring',), 10),\n",
       " (('second',), 10),\n",
       " (('over,',), 10),\n",
       " (('well',), 10),\n",
       " (('tale',), 10),\n",
       " (('her,',), 10),\n",
       " (('call',), 10),\n",
       " (('course,',), 10),\n",
       " (('tower.',), 10),\n",
       " (('except',), 10),\n",
       " (('hall',), 10),\n",
       " (('down.',), 10),\n",
       " (('mind,',), 10),\n",
       " (('was,',), 10),\n",
       " (('grown',), 10),\n",
       " (('mind.',), 10),\n",
       " (('piece',), 10),\n",
       " (('falling',), 10),\n",
       " (('all.',), 10),\n",
       " (('standing',), 10),\n",
       " (('devil-grass',), 10),\n",
       " (('miles',), 10),\n",
       " (('dying',), 10),\n",
       " (('kicked',), 10),\n",
       " (('remains',), 10),\n",
       " (('hadn’t',), 10),\n",
       " (('belly.',), 10),\n",
       " (('wild',), 10),\n",
       " (('shoulder.',), 10),\n",
       " (('laughed',), 10),\n",
       " (('trembling',), 10),\n",
       " (('talk',), 10),\n",
       " (('whatever',), 10),\n",
       " (('folded',), 10),\n",
       " (('right.”',), 10),\n",
       " (('“no.”',), 10),\n",
       " (('playing',), 10),\n",
       " (('leaning',), 10),\n",
       " (('on,',), 10),\n",
       " (('window',), 10),\n",
       " (('“that',), 10),\n",
       " (('meat',), 10),\n",
       " (('“don’t',), 10),\n",
       " (('fingers.',), 10),\n",
       " (('merely',), 10),\n",
       " (('this.',), 10),\n",
       " (('cast',), 10),\n",
       " (('wood',), 10),\n",
       " (('before.',), 10),\n",
       " (('played',), 10),\n",
       " (('softly.',), 10),\n",
       " (('broke',), 10),\n",
       " (('sky.',), 10),\n",
       " (('blind',), 10),\n",
       " (('it,”',), 10),\n",
       " (('handle',), 10),\n",
       " (('opened',), 10),\n",
       " (('times',), 10),\n",
       " (('“what’s',), 10),\n",
       " (('finally',), 10),\n",
       " (('“come',), 10),\n",
       " (('“my',), 10),\n",
       " (('hand,',), 10),\n",
       " (('ripped',), 10),\n",
       " (('drank',), 10),\n",
       " (('mother,',), 10),\n",
       " (('rails',), 10),\n",
       " (('ahead',), 10),\n",
       " (('step',), 10),\n",
       " (('“yes,”',), 10),\n",
       " (('cuthbert’s',), 10),\n",
       " (('west',), 10),\n",
       " (('leave',), 10),\n",
       " (('willow',), 10),\n",
       " (('universe',), 10),\n",
       " (('drawing',), 9),\n",
       " (('loved',), 9),\n",
       " (('running',), 9),\n",
       " (('in,',), 9),\n",
       " (('women',), 9),\n",
       " (('i’d',), 9),\n",
       " (('usually',), 9),\n",
       " (('middle',), 9),\n",
       " (('inside',), 9),\n",
       " (('supposed',), 9),\n",
       " (('grow',), 9),\n",
       " (('waited.',), 9),\n",
       " (('fact',), 9),\n",
       " (('thousand',), 9),\n",
       " (('hundred',), 9),\n",
       " (('please',), 9),\n",
       " (('occurred',), 9),\n",
       " (('live',), 9),\n",
       " (('hate',), 9),\n",
       " (('enjoy',), 9),\n",
       " (('keep',), 9),\n",
       " (('so,',), 9),\n",
       " (('everything',), 9),\n",
       " (('blade',), 9),\n",
       " (('hands,',), 9),\n",
       " (('third',), 9),\n",
       " (('pointed',), 9),\n",
       " (('little.',), 9),\n",
       " (('life',), 9),\n",
       " (('gone.',), 9),\n",
       " (('dust',), 9),\n",
       " (('pull',), 9),\n",
       " (('dreams',), 9),\n",
       " (('days',), 9),\n",
       " (('moment,',), 9),\n",
       " (('guess',), 9),\n",
       " (('“you’ll',), 9),\n",
       " (('head,',), 9),\n",
       " (('steps',), 9),\n",
       " (('dark.',), 9),\n",
       " (('table',), 9),\n",
       " (('ask.',), 9),\n",
       " (('“is',), 9),\n",
       " (('glow',), 9),\n",
       " (('dark,',), 9),\n",
       " (('floor',), 9),\n",
       " (('hostler',), 9),\n",
       " (('picked',), 9),\n",
       " (('me,”',), 9),\n",
       " (('eat',), 9),\n",
       " (('lit',), 9),\n",
       " (('boots',), 9),\n",
       " (('kept',), 9),\n",
       " (('steady',), 9),\n",
       " (('warm',), 9),\n",
       " (('hurt',), 9),\n",
       " (('“you’re',), 9),\n",
       " (('faced',), 9),\n",
       " (('neck',), 9),\n",
       " (('black,',), 9),\n",
       " (('allie,',), 9),\n",
       " (('sleep.',), 9),\n",
       " (('“yes.',), 9),\n",
       " (('seeing',), 9),\n",
       " (('“where',), 9),\n",
       " (('pump',), 9),\n",
       " (('himself.',), 9),\n",
       " (('“she',), 9),\n",
       " (('strong',), 9),\n",
       " (('off.',), 9),\n",
       " (('makes',), 9),\n",
       " (('twisted',), 9),\n",
       " (('kitchen',), 9),\n",
       " (('bring',), 9),\n",
       " (('stable',), 9),\n",
       " (('mrs.',), 9),\n",
       " (('it?',), 9),\n",
       " (('central',), 9),\n",
       " (('guard',), 9),\n",
       " (('river',), 9),\n",
       " (('books',), 8),\n",
       " (('nineteen',), 8),\n",
       " (('number',), 8),\n",
       " (('suppose',), 8),\n",
       " (('late',), 8),\n",
       " (('question.',), 8),\n",
       " (('one’s',), 8),\n",
       " (('then.',), 8),\n",
       " (('forever',), 8),\n",
       " (('twenty',), 8),\n",
       " (('pretty',), 8),\n",
       " (('best',), 8),\n",
       " (('did.',), 8),\n",
       " (('you’ve',), 8),\n",
       " (('novel',), 8),\n",
       " (('tower,',), 8),\n",
       " (('saying',), 8),\n",
       " (('couldn’t',), 8),\n",
       " (('question',), 8),\n",
       " (('real',), 8),\n",
       " (('hardly',), 8),\n",
       " (('hell',), 8),\n",
       " (('year',), 8),\n",
       " (('volume',), 8),\n",
       " (('you’ll',), 8),\n",
       " (('finished',), 8),\n",
       " (('years.',), 8),\n",
       " (('begin',), 8),\n",
       " (('spring',), 8),\n",
       " (('world.',), 8),\n",
       " (('door.',), 8),\n",
       " (('alone',), 8),\n",
       " (('mother’s',), 8),\n",
       " (('sweet',), 8),\n",
       " (('occasional',), 8),\n",
       " (('sign',), 8),\n",
       " (('track',), 8),\n",
       " (('swung',), 8),\n",
       " (('bit',), 8),\n",
       " (('others.',), 8),\n",
       " (('short',), 8),\n",
       " (('fire,',), 8),\n",
       " (('straight',), 8),\n",
       " (('so.',), 8),\n",
       " (('grass,',), 8),\n",
       " (('stay',), 8),\n",
       " (('slept.',), 8),\n",
       " (('in.',), 8),\n",
       " (('coach',), 8),\n",
       " (('break',), 8),\n",
       " (('you,”',), 8),\n",
       " (('zoltan',), 8),\n",
       " (('horse',), 8),\n",
       " (('too.',), 8),\n",
       " (('drop',), 8),\n",
       " (('water,',), 8),\n",
       " (('named',), 8),\n",
       " (('dull',), 8),\n",
       " (('asked,',), 8),\n",
       " (('open.',), 8),\n",
       " (('man.',), 8),\n",
       " (('day.',), 8),\n",
       " (('dust.',), 8),\n",
       " (('dim',), 8),\n",
       " (('loose',), 8),\n",
       " (('forward,',), 8),\n",
       " (('answer.',), 8),\n",
       " (('broken',), 8),\n",
       " (('bar',), 8),\n",
       " (('wore',), 8),\n",
       " (('forehead.',), 8),\n",
       " (('meant',), 8),\n",
       " (('nodded,',), 8),\n",
       " (('smiling',), 8),\n",
       " (('color',), 8),\n",
       " (('aware',), 8),\n",
       " (('push',), 8),\n",
       " (('staring',), 8),\n",
       " (('itself.',), 8),\n",
       " (('“no,”',), 8),\n",
       " (('worn',), 8),\n",
       " (('hood',), 8),\n",
       " (('mixed',), 8),\n",
       " (('within',), 8),\n",
       " (('stick',), 8),\n",
       " (('knowing',), 8),\n",
       " (('path',), 8),\n",
       " (('grew',), 8),\n",
       " (('circle',), 8),\n",
       " (('bed',), 8),\n",
       " (('place.',), 8),\n",
       " (('closer',), 8),\n",
       " (('skin',), 8),\n",
       " (('screaming',), 8),\n",
       " (('demon',), 8),\n",
       " (('wall',), 8),\n",
       " (('ironwood',), 8),\n",
       " (('beat',), 8),\n",
       " (('peered',), 8),\n",
       " (('shoulder',), 8),\n",
       " (('fired',), 8),\n",
       " (('forehead',), 8),\n",
       " (('sent',), 8),\n",
       " (('“let’s',), 8),\n",
       " (('rise',), 8),\n",
       " (('cook',), 8),\n",
       " (('susan',), 8),\n",
       " (('wished',), 8),\n",
       " (('giant',), 8),\n",
       " (('penguin',), 7),\n",
       " (('street,',), 7),\n",
       " (('filling',), 7),\n",
       " (('page',), 7),\n",
       " (('one,',), 7),\n",
       " (('written',), 7),\n",
       " (('truth',), 7),\n",
       " (('world,',), 7),\n",
       " (('pocket',), 7),\n",
       " (('nineteen.',), 7),\n",
       " (('road',), 7),\n",
       " (('i’ve',), 7),\n",
       " (('beer',), 7),\n",
       " (('dream',), 7),\n",
       " (('anyway.',), 7),\n",
       " (('sit',), 7),\n",
       " (('no,',), 7),\n",
       " (('realized',), 7),\n",
       " (('volumes',), 7),\n",
       " (('growing',), 7),\n",
       " (('side.',), 7),\n",
       " (('heads',), 7),\n",
       " (('finish',), 7),\n",
       " (('sick',), 7),\n",
       " (('landed',), 7),\n",
       " (('death',), 7),\n",
       " (('remained',), 7),\n",
       " (('sharp',), 7),\n",
       " (('anyone',), 7),\n",
       " (('period',), 7),\n",
       " (('particularly',), 7),\n",
       " (('telling',), 7),\n",
       " (('deal',), 7),\n",
       " (('hidden',), 7),\n",
       " (('surprised',), 7),\n",
       " (('matter',), 7),\n",
       " (('forgotten',), 7),\n",
       " (('faces.',), 7),\n",
       " (('desert,',), 7),\n",
       " (('death.',), 7),\n",
       " (('hide',), 7),\n",
       " (('carefully',), 7),\n",
       " (('looped',), 7),\n",
       " (('buried',), 7),\n",
       " (('ground',), 7),\n",
       " (('blow',), 7),\n",
       " (('worlds',), 7),\n",
       " (('heat.',), 7),\n",
       " (('hill',), 7),\n",
       " (('scrawny',), 7),\n",
       " (('while.',), 7),\n",
       " (('low',), 7),\n",
       " (('stones',), 7),\n",
       " (('well.',), 7),\n",
       " (('tull,',), 7),\n",
       " (('right,',), 7),\n",
       " (('spread',), 7),\n",
       " (('“tell',), 7),\n",
       " (('expected',), 7),\n",
       " (('brief',), 7),\n",
       " (('halfway',), 7),\n",
       " (('offered',), 7),\n",
       " (('come.',), 7),\n",
       " (('unless',), 7),\n",
       " (('night.',), 7),\n",
       " (('neither',), 7),\n",
       " (('sky,',), 7),\n",
       " (('dead.',), 7),\n",
       " (('main',), 7),\n",
       " (('tull.',), 7),\n",
       " (('bodies',), 7),\n",
       " (('crashed',), 7),\n",
       " (('old,',), 7),\n",
       " (('grasped',), 7),\n",
       " (('outside',), 7),\n",
       " (('floor,',), 7),\n",
       " (('approached',), 7),\n",
       " (('cracked',), 7),\n",
       " (('sees',), 7),\n",
       " (('player',), 7),\n",
       " (('everything.',), 7),\n",
       " (('breathing',), 7),\n",
       " (('grass.',), 7),\n",
       " (('blood.',), 7),\n",
       " (('over.',), 7),\n",
       " (('sleep.”',), 7),\n",
       " (('afternoon',), 7),\n",
       " (('wake',), 7),\n",
       " (('flying',), 7),\n",
       " (('corner,',), 7),\n",
       " (('soft',), 7),\n",
       " (('“get',), 7),\n",
       " (('right,”',), 7),\n",
       " (('leaving',), 7),\n",
       " (('grinned',), 7),\n",
       " (('grinning',), 7),\n",
       " (('waiting',), 7),\n",
       " (('sitting',), 7),\n",
       " (('later,',), 7),\n",
       " (('word.',), 7),\n",
       " (('o’',), 7),\n",
       " (('rolling',), 7),\n",
       " (('getting',), 7),\n",
       " (('you’d',), 7),\n",
       " (('pain',), 7),\n",
       " (('now,”',), 7),\n",
       " (('weight',), 7),\n",
       " (('ten',), 7),\n",
       " (('was.',), 7),\n",
       " (('quick',), 7),\n",
       " (('walking',), 7),\n",
       " (('dressed',), 7),\n",
       " (('perfectly',), 7),\n",
       " (('end,',), 7),\n",
       " (('marten,',), 7),\n",
       " (('jamie',), 7),\n",
       " (('smiled.',), 7),\n",
       " (('granite',), 7),\n",
       " (('trees',), 7),\n",
       " (('jawbone',), 7),\n",
       " (('good.',), 7),\n",
       " (('mechanical',), 7),\n",
       " (('power',), 7),\n",
       " (('darkness.',), 7),\n",
       " (('metal',), 7),\n",
       " (('dead,',), 6),\n",
       " (('business',), 6),\n",
       " (('part,',), 6),\n",
       " (('lands',), 6),\n",
       " (('glass',), 6),\n",
       " (('bones',), 6),\n",
       " (('bullet',), 6),\n",
       " (('silence',), 6),\n",
       " (('facing',), 6),\n",
       " (('following',), 6),\n",
       " (('fallen',), 6),\n",
       " (('street.',), 6),\n",
       " (('rotten',), 6),\n",
       " (('cared',), 6),\n",
       " (('use',), 6),\n",
       " (('met',), 6),\n",
       " (('he’ll',), 6),\n",
       " (('off,',), 6),\n",
       " (('size',), 6),\n",
       " (('there’s',), 6),\n",
       " (('ii',), 6),\n",
       " (('possible',), 6),\n",
       " (('liked',), 6),\n",
       " (('showed',), 6),\n",
       " (('contained',), 6),\n",
       " (('talking',), 6),\n",
       " (('size.',), 6),\n",
       " (('added',), 6),\n",
       " (('iii',), 6),\n",
       " (('somehow',), 6),\n",
       " (('lines',), 6),\n",
       " (('knocked',), 6),\n",
       " (('done,',), 6),\n",
       " (('done.',), 6),\n",
       " (('hope',), 6),\n",
       " (('life.',), 6),\n",
       " (('readers',), 6),\n",
       " (('died',), 6),\n",
       " (('ones.',), 6),\n",
       " (('before,',), 6),\n",
       " (('remain',), 6),\n",
       " (('years,',), 6),\n",
       " (('three.',), 6),\n",
       " (('course',), 6),\n",
       " (('wants',), 6),\n",
       " (('removed',), 6),\n",
       " (('usual',), 6),\n",
       " (('do.',), 6),\n",
       " (('slightly',), 6),\n",
       " (('earth.',), 6),\n",
       " (('cloudy',), 6),\n",
       " (('hat',), 6),\n",
       " (('stretched',), 6),\n",
       " (('crumbled',), 6),\n",
       " (('oddly',), 6),\n",
       " (('answer',), 6),\n",
       " (('bitter',), 6),\n",
       " (('camp',), 6),\n",
       " (('letting',), 6),\n",
       " (('wind.',), 6),\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:54:18.043109Z",
     "start_time": "2017-11-03T19:54:18.039911Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_books = db.books.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:36.658485Z",
     "start_time": "2017-11-04T22:51:36.654272Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=['title','isbn','text'])\n",
    "# for book in all_books:\n",
    "#     df2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:36.859492Z",
     "start_time": "2017-11-04T22:51:36.854045Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book_counts = {}\n",
    "# for book in all_books:\n",
    "#     title,counts = book_word_count(book,1)\n",
    "#     book_counts[title] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:37.085862Z",
     "start_time": "2017-11-04T22:51:37.081933Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean_books = []\n",
    "# for book in all_books:\n",
    "#     text = clean_text(book)\n",
    "#     clean_books.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:41.383281Z",
     "start_time": "2017-11-04T22:51:41.380214Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book_list = sorted(book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:43.830724Z",
     "start_time": "2017-11-04T22:51:43.827125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_books = ['Gunslinger, The','Drawing of the Three, The','Waste Lands, The',\n",
    "            'Wizard and Glass','Wolves of the Calla','Song of Susannah','Dark Tower, The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T21:34:52.276435Z",
     "start_time": "2017-11-02T21:34:52.272037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gunslinger, The', 'Drawing of the Three, The', 'Waste Lands, The', 'Wizard and Glass', 'Wolves of the Calla', 'Song of Susannah', 'Dark Tower, The']\n"
     ]
    }
   ],
   "source": [
    "print(dt_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:46.255349Z",
     "start_time": "2017-11-04T22:51:46.251254Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_books = db.books.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:50.018258Z",
     "start_time": "2017-11-04T22:51:50.014685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_text = []\n",
    "# for book in all_books:\n",
    "#     text = clean_text(book)\n",
    "#     all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:51:01.248141Z",
     "start_time": "2017-11-04T22:51:01.242598Z"
    }
   },
   "outputs": [],
   "source": [
    "# dt_text = []\n",
    "# for book in all_books:\n",
    "#     if book['title'] in dt_books:\n",
    "#         text  = clean_text(book)\n",
    "#         name = book['title']\n",
    "#         print(book['title'])\n",
    "#         dt_text.append(text)\n",
    "#     #else:\n",
    "#         #print(\"Haha Fuck you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:50:15.265374Z",
     "start_time": "2017-11-03T19:50:15.260374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:51:16.442046Z",
     "start_time": "2017-11-03T19:51:14.140428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "(29598, 7)\n"
     ]
    }
   ],
   "source": [
    "df,corp,id2word = book_cv(dt_text,stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {
    "height": "669px",
    "left": "0px",
    "right": "1228px",
    "top": "90px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
